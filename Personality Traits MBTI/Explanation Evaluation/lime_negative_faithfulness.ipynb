{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 01:32:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.78                 Driver Version: 551.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P0             23W /  128W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('../datasets')\n",
    "from mbti_500 import getDataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model name\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
    "\n",
    "# setting the dataset\n",
    "dataset='MBTI 500 multi_label.csv'\n",
    "\n",
    "\n",
    "# setting the data path\n",
    "if os.path.exists(f'/datasets/mbti/{dataset}'):\n",
    "    DATAPATH=f'/datasets/mbti/{dataset}'\n",
    "else:\n",
    "    DATAPATH=f'../data/{dataset}'\n",
    "\n",
    "# setting the checkpoint path \n",
    "if os.path.exists('ckpts'):\n",
    "    CHECKPOINTPATH = 'ckpts/Persnality_MBTI'\n",
    "else:\n",
    "    CHECKPOINTPATH = '../ckpts/Persnality_MBTI'\n",
    "\n",
    "# training parameters\n",
    "MAX_LEN = 512\n",
    "\n",
    "# TOKENIZER\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# setting the device\n",
    "device = \"cpu\"\n",
    "\n",
    "# setting the random seed\n",
    "torch.manual_seed(99)\n",
    "torch.cuda.manual_seed(99)\n",
    "torch.cuda.manual_seed_all(99)\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/MBTI 500 multi_label.csv', '../ckpts/Persnality_MBTI', 'cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAPATH,CHECKPOINTPATH,device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "      <th>IE_true</th>\n",
       "      <th>NS_true</th>\n",
       "      <th>TF_true</th>\n",
       "      <th>JP_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diffiuc get sense without actually rift idea l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quiet one people like reason hard look like ey...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think video book anyday however problem qualit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seatle look like alright place love like frien...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>understand sometimes need immensely private go...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   IE   NS   TF   JP  \\\n",
       "0  diffiuc get sense without actually rift idea l...  0.0  0.0  0.0  1.0   \n",
       "1  quiet one people like reason hard look like ey...  1.0  0.0  0.0  0.0   \n",
       "2  think video book anyday however problem qualit...  0.0  0.0  0.0  1.0   \n",
       "3  seatle look like alright place love like frien...  0.0  0.0  0.0  0.0   \n",
       "4  understand sometimes need immensely private go...  0.0  0.0  1.0  1.0   \n",
       "\n",
       "   IE_true  NS_true  TF_true  JP_true  \n",
       "0      1.0      0.0      0.0      1.0  \n",
       "1      1.0      0.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0      1.0  \n",
       "3      0.0      0.0      0.0      0.0  \n",
       "4      0.0      0.0      1.0      0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the test data with the predictions\n",
    "df=pd.read_csv('save_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get First 100 mispredictions with equal distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text        0\n",
       " IE         50\n",
       " NS         96\n",
       " TF         57\n",
       " JP         23\n",
       " IE_true    50\n",
       " NS_true    89\n",
       " TF_true    62\n",
       " JP_true    23\n",
       " dtype: int64,\n",
       " text        0\n",
       " IE         50\n",
       " NS          4\n",
       " TF         43\n",
       " JP         77\n",
       " IE_true    50\n",
       " NS_true    11\n",
       " TF_true    38\n",
       " JP_true    77\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 100 mispredicted samples IE samples\n",
    "exctracted_misprediction_IE = df[df['IE'] != df['IE_true']]\n",
    "# get 100 mispredicted samples IE samples with equal number of 1,0\n",
    "exctracted_misprediction_IE_1 = exctracted_misprediction_IE[exctracted_misprediction_IE['IE']==1].head(50)\n",
    "exctracted_misprediction_IE_0 = exctracted_misprediction_IE[exctracted_misprediction_IE['IE']==0].head(50)\n",
    "exctracted_misprediction_IE = pd.concat([exctracted_misprediction_IE_1,exctracted_misprediction_IE_0])\n",
    "exctracted_misprediction_IE = exctracted_misprediction_IE.sample(frac=1).reset_index(drop=True)\n",
    "count_0= exctracted_misprediction_IE.eq(0).sum()\n",
    "count_1= exctracted_misprediction_IE.eq(1).sum()\n",
    "count_0,count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 mispredicted samples NS samples\n",
    "exctracted_misprediction_NS = df[df['NS'] != df['NS_true']]\n",
    "exctracted_misprediction_NS_1 = exctracted_misprediction_NS[exctracted_misprediction_NS['NS']==1].head(50)\n",
    "exctracted_misprediction_NS_0 = exctracted_misprediction_NS[exctracted_misprediction_NS['NS']==0].head(50)\n",
    "exctracted_misprediction_NS = pd.concat([exctracted_misprediction_NS_1,exctracted_misprediction_NS_0])\n",
    "exctracted_misprediction_NS = exctracted_misprediction_NS.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text        0\n",
       " IE         83\n",
       " NS         50\n",
       " TF         47\n",
       " JP         37\n",
       " IE_true    72\n",
       " NS_true    50\n",
       " TF_true    63\n",
       " JP_true    38\n",
       " dtype: int64,\n",
       " text        0\n",
       " IE         17\n",
       " NS         50\n",
       " TF         53\n",
       " JP         63\n",
       " IE_true    28\n",
       " NS_true    50\n",
       " TF_true    37\n",
       " JP_true    62\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_0= exctracted_misprediction_NS.eq(0).sum()\n",
    "count_1= exctracted_misprediction_NS.eq(1).sum()\n",
    "count_0,count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 mispredicted samples TF samples\n",
    "exctracted_misprediction_TF = df[df['TF'] != df['TF_true']]\n",
    "exctracted_misprediction_TF_1 = exctracted_misprediction_TF[exctracted_misprediction_TF['TF']==1].head(50)\n",
    "exctracted_misprediction_TF_0 = exctracted_misprediction_TF[exctracted_misprediction_TF['TF']==0].head(50)\n",
    "exctracted_misprediction_TF = pd.concat([exctracted_misprediction_TF_1,exctracted_misprediction_TF_0])\n",
    "exctracted_misprediction_TF = exctracted_misprediction_TF.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text        0\n",
       " IE         89\n",
       " NS         94\n",
       " TF         50\n",
       " JP         35\n",
       " IE_true    76\n",
       " NS_true    88\n",
       " TF_true    50\n",
       " JP_true    45\n",
       " dtype: int64,\n",
       " text        0\n",
       " IE         11\n",
       " NS          6\n",
       " TF         50\n",
       " JP         65\n",
       " IE_true    24\n",
       " NS_true    12\n",
       " TF_true    50\n",
       " JP_true    55\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_0= exctracted_misprediction_TF.eq(0).sum()\n",
    "count_1= exctracted_misprediction_TF.eq(1).sum()\n",
    "count_0,count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get 100 mispredicted samples JP samples\n",
    "exctracted_misprediction_JP = df[df['JP'] != df['JP_true']]\n",
    "exctracted_misprediction_JP_1 = exctracted_misprediction_JP[exctracted_misprediction_JP['JP']==1].head(50)\n",
    "exctracted_misprediction_JP_0 = exctracted_misprediction_JP[exctracted_misprediction_JP['JP']==0].head(50)\n",
    "exctracted_misprediction_JP = pd.concat([exctracted_misprediction_JP_1,exctracted_misprediction_JP_0])\n",
    "exctracted_misprediction_JP = exctracted_misprediction_JP.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text        0\n",
       " IE         96\n",
       " NS         99\n",
       " TF         57\n",
       " JP         50\n",
       " IE_true    84\n",
       " NS_true    91\n",
       " TF_true    62\n",
       " JP_true    50\n",
       " dtype: int64,\n",
       " text        0\n",
       " IE          4\n",
       " NS          1\n",
       " TF         43\n",
       " JP         50\n",
       " IE_true    16\n",
       " NS_true     9\n",
       " TF_true    38\n",
       " JP_true    50\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_0= exctracted_misprediction_JP.eq(0).sum()\n",
    "count_1= exctracted_misprediction_JP.eq(1).sum()\n",
    "count_0,count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = ['IE','NS','TF','JP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy of the extracted_df\n",
    "def get_accuracy_extracted_df(df,label):\n",
    "    accuracy = {}\n",
    "    accuracy[label] = (df[label]==df[label+'_true']).sum()/len(df)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'IE': 0.0}, {'NS': 0.0}, {'TF': 0.0}, {'JP': 0.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_extracted_df(exctracted_misprediction_IE,\"IE\"),get_accuracy_extracted_df(exctracted_misprediction_NS,\"NS\"),get_accuracy_extracted_df(exctracted_misprediction_TF,\"TF\"),get_accuracy_extracted_df(exctracted_misprediction_JP,\"JP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "import sys\n",
    "sys.path.append('../Models')\n",
    "import MBTI_model_lime as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base_no_words loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2433a658d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_words = model.ROBERTAClass(PRE_TRAINED_MODEL_NAME)\n",
    "model_no_words.load_state_dict(torch.load(CHECKPOINTPATH + f'_clean_Best_{PRE_TRAINED_MODEL_NAME}.bin', map_location=torch.device(device)))\n",
    "model_no_words.to(device)\n",
    "print(f'{PRE_TRAINED_MODEL_NAME}_no_words loaded')\n",
    "model_no_words.eval()\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "lime_explanation_IE= model.explain_model(model_no_words, exctracted_misprediction_IE[\"text\"],aspect='IE')\n",
    "# save the explanation to a pkl file\n",
    "with open('lime_explanation_IE.pkl', 'wb') as f:\n",
    "    pkl.dump(lime_explanation_IE, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explanation_NS= model.explain_model(model_no_words, exctracted_misprediction_NS[\"text\"],aspect='NS')\n",
    "# save the explanation to a pkl file\n",
    "with open('lime_explanation_NS.pkl', 'wb') as f:\n",
    "    pkl.dump(lime_explanation_NS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explanation_TF= model.explain_model(model_no_words, exctracted_misprediction_TF[\"text\"],aspect='TF')\n",
    "# save the explanation to a pkl file\n",
    "with open('lime_explanation_TF.pkl', 'wb') as f:\n",
    "    pkl.dump(lime_explanation_TF, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explanation_JP= model.explain_model(model_no_words, exctracted_misprediction_JP[\"text\"],aspect='JP')\n",
    "# save the explanation to a pkl file\n",
    "with open('lime_explanation_JP.pkl', 'wb') as f:\n",
    "    pkl.dump(lime_explanation_JP, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the top 100 features and check the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to the extracted_df that contains with the text with the most important 100 tokens removed \n",
    "def remove_words(text,words):\n",
    "    for word in words:\n",
    "        text = text.replace(word,'')\n",
    "        # remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_100_tokens(lime_explanations,aspect,extracted_df):\n",
    "    for i in range(len(lime_explanations)):\n",
    "        # sort the words by importance\n",
    "        sorted_explanation = sorted(lime_explanations[i].as_list(),key=lambda x: x[1],reverse=True)\n",
    "        words = [word for word,weight in sorted_explanation[:100]]\n",
    "        extracted_df.loc[i,aspect+'_no_words'] = remove_words(extracted_df.loc[i,'text'],words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_100_tokens(lime_explanation_IE,\"IE\",exctracted_misprediction_IE)\n",
    "remove_100_tokens(lime_explanation_NS,\"NS\",exctracted_misprediction_NS)\n",
    "remove_100_tokens(lime_explanation_TF,\"TF\",exctracted_misprediction_TF)\n",
    "remove_100_tokens(lime_explanation_JP,\"JP\",exctracted_misprediction_JP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes\n",
    "exctracted_misprediction_IE.to_csv('exctracted_misprediction_IE.csv',index=False)\n",
    "exctracted_misprediction_NS.to_csv('exctracted_misprediction_NS.csv',index=False)\n",
    "exctracted_misprediction_TF.to_csv('exctracted_misprediction_TF.csv',index=False)\n",
    "exctracted_misprediction_JP.to_csv('exctracted_misprediction_JP.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes\n",
    "exctracted_misprediction_IE = pd.read_csv('exctracted_misprediction_IE.csv')\n",
    "exctracted_misprediction_NS = pd.read_csv('exctracted_misprediction_NS.csv')\n",
    "exctracted_misprediction_TF = pd.read_csv('exctracted_misprediction_TF.csv')\n",
    "exctracted_misprediction_JP = pd.read_csv('exctracted_misprediction_JP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the performance metrics for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "import sys\n",
    "sys.path.append('../Models')\n",
    "import roberta_mbti as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base_no_words loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x17020dfe750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model = model.ROBERTAClass(PRE_TRAINED_MODEL_NAME)\n",
    "roberta_model.load_state_dict(torch.load(CHECKPOINTPATH + f'_clean_Best_{PRE_TRAINED_MODEL_NAME}.bin', map_location=torch.device(device)))\n",
    "roberta_model.to(device)\n",
    "print(f'{PRE_TRAINED_MODEL_NAME}_no_words loaded')\n",
    "roberta_model.eval()\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions for the extracted_df IE_no_words , NS_no_words , TF_no_words , JP_no_words\n",
    "IE_no_words_pred=roberta_model.getPrediction(exctracted_misprediction_IE['IE_no_words'].to_list())\n",
    "NS_no_words_pred=roberta_model.getPrediction(exctracted_misprediction_NS['NS_no_words'].to_list())\n",
    "TF_no_words_pred=roberta_model.getPrediction(exctracted_misprediction_TF['TF_no_words'].to_list())\n",
    "JP_no_words_pred=roberta_model.getPrediction(exctracted_misprediction_JP['JP_no_words'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the predictions\n",
    "IE_no_words_pred = np.round(IE_no_words_pred)\n",
    "NS_no_words_pred = np.round(NS_no_words_pred)\n",
    "TF_no_words_pred = np.round(TF_no_words_pred)\n",
    "JP_no_words_pred = np.round(JP_no_words_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions of the extracted_df IE_no_words , NS_no_words , TF_no_words , JP_no_words for each of the aspects\n",
    "exctracted_misprediction_IE['IE_no_words_pred'] = IE_no_words_pred[:,0]\n",
    "exctracted_misprediction_NS['NS_no_words_pred'] = NS_no_words_pred[:,1]\n",
    "exctracted_misprediction_TF['TF_no_words_pred'] = TF_no_words_pred[:,2]\n",
    "exctracted_misprediction_JP['JP_no_words_pred'] = JP_no_words_pred[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy of the extracted_df\n",
    "def get_accuracy_extracted_df_after(df,label):\n",
    "    accuracy = {}\n",
    "    accuracy[label] = (df[label+'_no_words_pred']==df[label+'_true']).sum()/len(df)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'IE': 0.4}, {'NS': 0.5}, {'TF': 0.5}, {'JP': 0.44})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_extracted_df_after(exctracted_misprediction_IE,\"IE\"),get_accuracy_extracted_df_after(exctracted_misprediction_NS,\"NS\"),get_accuracy_extracted_df_after(exctracted_misprediction_TF,\"TF\"),get_accuracy_extracted_df_after(exctracted_misprediction_JP,\"JP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# save the accuracy increase\n",
    "accuracy_scores = {}\n",
    "accuracy_scores['IE'] = get_accuracy_extracted_df_after(exctracted_misprediction_IE,\"IE\")\n",
    "accuracy_scores['NS'] = get_accuracy_extracted_df_after(exctracted_misprediction_NS,\"NS\")\n",
    "accuracy_scores['TF'] = get_accuracy_extracted_df_after(exctracted_misprediction_TF,\"TF\")\n",
    "accuracy_scores['JP'] = get_accuracy_extracted_df_after(exctracted_misprediction_JP,\"JP\")\n",
    "\n",
    "with open('accuracy_scores.json', 'w') as f:\n",
    "    json.dump(accuracy_scores, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
