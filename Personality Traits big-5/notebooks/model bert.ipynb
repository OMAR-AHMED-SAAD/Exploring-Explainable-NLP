{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# check if local env or paperspace\n",
    "import os\n",
    "if os.path.exists('/datasets/essays/essays_cleaned.csv'):\n",
    "    dataset_path='/datasets/essays/essays_cleaned.csv'\n",
    "else:\n",
    "    dataset_path='../data/essays_cleaned.csv'\n",
    "df = pd.read_csv(dataset_path,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "# Hyperparameters\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid\n",
    "# intialize tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, df, max_len,target_list):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = list(df['TEXT'])\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.targets = self.df[target_list].values\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        text_length = len(text)\n",
    "        middle_start = text_length // 3\n",
    "        middle_end = 2 * (text_length // 3)\n",
    "        text_middle = text[middle_start:middle_end]\n",
    "        text=text_middle\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        print(self.targets[idx])\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target list is the list of big 5 personality traits that we want to predict\n",
    "target_list= list(df.columns[1:])\n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_dataset = EssayDataset(df_train, MAX_LEN, target_list)\n",
    "valid_dataset = EssayDataset(df_valid, MAX_LEN, target_list)\n",
    "test_dataset = EssayDataset(df_test, MAX_LEN, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0]\n",
      "[0 1 0 0 0]\n",
      "[1 0 1 1 0]\n",
      "[0 1 1 0 1]\n",
      "[0 0 0 1 1]\n",
      "[0 1 0 0 0]\n",
      "[0 0 1 0 1]\n",
      "[0 0 0 0 0]\n",
      "[0 0 1 1 1]\n",
      "[1 0 1 1 0]\n",
      "[1 1 1 1 1]\n",
      "[0 1 1 0 1]\n",
      "[1 0 1 0 0]\n",
      "[0 0 0 0 0]\n",
      "[0 1 1 1 1]\n",
      "[1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['o Ill write that This assignment is very weird and I do not know how anything can come of it What should I say  I do not consciously know what I am thinking so I do not know what to type I Am just sitting here trying to think of something profound to say That is stupid considering that I do not have to say anything important but I guess that is what I Am used to doing when I write an assignment Right now I Am staring at a picture of a whales tale as it dives into the ocean It really looks kind of stupid and I cannot think of anything else to say about it You psychology people are strange and I wonder what the heck you are going t',\n",
       "  ' weird  sleep is good that girl next to me was hot today forgot her name though  oh well ill ask colin dude needs to shave funny though wonder what linda is doing I need to call her why upgrade things 13 minutes left SO long this is not acoustic liars oooh wrong song  what is up with internet downloading its there for a reason CDs are so damn expensive pppppffff so tired this canker if that is how you spell it sore hurts I need some stuff for it I wonder if its stress doing that oh well  pennebaker reminds me of hammermesh same room same sense of humor vitris is cool funny dude I miss irving friends who is this people are weird you do not talk to someone and not know who they are oooooh ok that is who weirdo people getting my screen name high school sucked crappy ugly school so many Aholes remember waiting  the ataris are awesome so I cannot type I need to play hockey no money though ooooh let us play NCAA I kicked Kstates ass OU is going to die I want tickets so bad sell them back 8 minutes wonder what he is holding I need to put that together THAT IS NOT EVEN A SONG girl in our dorm who is that sounds hot I like pie too yummmmm apple hahaha Homer funny guy what is the name of that sho',\n",
       "  'one else in my life This computer is annoying I hate computers that are not like the one I have at home and that I know I miss being able to chat online to Amy and steph and everyone else I wish I could go to Canada just for the hell of it I Am probably not the girl he would be looking for he seems like the buff good looking ty0pe that everyone knows oh well My stomach keeps making these nasty noises whish gets embarrassing in class I Am worried about school work I know that I just need to keep on top of things and Ill be less stressed out and stuff I just n4ed to get up tomorrow  go to class then come back and do my Latin and then some math or something  Then do my bio after Latin and go to the discussion Ill go run those errands and make those phone calls maybe marissa will do them with me so we can hang if not Ill call christina or someone I want to get involved with the Wesly ',\n",
       "  'm Ben my exboyfriend Not going to waste my time on him anymore why do people cheat on each other I woant  ever understand that And what makes him think that just by sayng I Love You htat I am just going to forget everything Forget hijm  at  least I have met someone new that I can tlak to heis so sweet and all but I think I should call it off nothing ever works for me  and I ned to concentrate more on my shcool work I do not know what to do about guys I need to meet some more girls to hang out  with but the only ones I see are all snoby sorority girls  a nd that just is not me at all Kyle is such a sweet guy and all but should  I tell him htat I need to slow down and work on my shcool or should I just let things go and see what happens I Am scared to just go with  the flow I really like this guy a lot and the last time I felt this way I got hurt and  that pain is still here Everytimt Ben calls I  get a ',\n",
       "  'men and I Am really worried about UT because of the horror stories all my upperclassmen friends told me about My father told me to concentrate on my studies because it is my responsibility to do excellent in school He said that it is my life that I Am preparing for not his I Am also using my money because I Am using the loan that I received I Have been dating my girlfriend for two and half months and I think were doing good Everything is working just right I cannot wait till this weekend so I do not have to worry about school for about two days Actually I have to do s',\n",
       "  'er frustrates me because for some reason i always start crying when she lectures me  i have NO CLUE WHY but that really freaked out my roommate  she is never seen me like that before but its ok she was really cool about it and there are more to come she she better get used to it  this is nice that i live with her yes  so the dust is really getting to me i am choking up hairballs all over the place i went to my RA but she was not there and the front desk said that we have to go downstairs to fill out a maintenance request form well do that when we go have dinner dinner here at dobie really is not worth the amount that we pay to stay here its decent  but i expected better my hands are starting to hurt now I Have never typed this much straight before this better b good for my forearms damn man i need to go back to the gym my roommate is done praying now i bet she is looking over my shoulder  she is such an eavesdropper dude  ok now i pisseed her off  hehe jk jk  she really really wanted to do this assignement with me even though she is not in psych its really interseting to her but it is to me too  i really like this class  ok  this sounds like total sucking up but do not worry i hate sucking up  i am totally honest  it gets me into trouble sometimes  i do not know who i am sposed to b typing to or if i am not sposed to b talking to someone at all  well its ok  its stream of conciousness rite rite i could go for some starbucks rite now  i love that place  i think with all the exercise i get from the walkin all over this freaking campus balances with the junk i eat  but i think that for a college student  i eat pretty healthily like i eat fruit and bananas and i love milk  ok i know no one cares any how  back to being homesick the only thing i miss is the familiarity and the friends but this will change  austin is steadily becoming more familiar to me and my friends arenot at home anyway  they all go to other schools you know  i really like how i have a movie theatre downstairs from my dorm i wa',\n",
       "  'se Jester SUCKS And the food is nothing special at all I got to ask mom and dad for some cash I really cannot wait to get back to Houston to see everybody there and especially Dean Cassie Mrs Smith and of course Christie I need to keep working out Its doing me alot of good Carter needs to tone it down in there so that I can finish this paper Holy cow my mind just went blank Oh wait a minute 2 great looking girls are swimming over at the Theta house I hope t',\n",
       "  'oved me even though he really and truly loved me a lot sometimes  I wonder if he is gay I know that sounds odd but maybe    no forget it In so exhausted and all I want to do is go to sleep in my own bed under a real homy roof I know it sounds funny becaouse they are probably all asleep but I wonder what they are doing mY family and John and my other long lost friends who off at some far away college poor Jonathan is out at sea I hope its not raining on him THe food here is good but I do not like having some one tell me when I have to eat and when I cannot There is no refrigerator with Blue Bell stuffed in the back And most of all there is no Arlington Dance Academy Oh guys I wish I was there to teach my little kids and have them run up and hug my leg I wish I had my senior class back because I have so many great ideas from new pop songs I Have heard here in Austin Why cannot I just skip the whole college thing I know ican do it and I have the abiltiy and the committment to finish Ijust do not th',\n",
       "  'is what I just said she is a hobbitgirl I like it when people breakdance I regret that I have only one blah to give for my country all my friends talk about is records and porn I Am trying to hide the fact that I like ska and very confused about what I should be I only pretend to know everything really I do not believe anything its easier that way I do not like pepsi ut gives me pimples and blisters I do not want to get my wisdom teeth extracted because I do not want to like medicine',\n",
       "  ' am in classes just chemistry and Calculus I want to call to talk to my girlfriend I wqent home to houston and saw her last weekend I have not spoken to her since monday and I really want to i have clculus I need to be working on on top of this but it isnot do until wednesday so this assignmnet is more pressing Sometimes I woory about whether or not I am going to be able to hackk it in some of my classes once we have some tests chemistry and calculus are pretty hard I probably chose about the hardest major thjough Electrical Engineering I keep on think ing of songs in my head but that cannot well be written caan it I enjoy music both listening and making music Playing guitar is one of the major strees relievers for me I do it at least 30 minutes a day now that I am in college I used to only play that much a week I Have been writing more songs now too I had not written any for a while so I hasd a clear fresh mind to work off of I have been devoting most ',\n",
       "  ' and take advantage of you I cannot believe there is a theme I thought we grew out of it in high school I do not like the idea of some of the things she wants to do I do not know if she is naive or if she is pretending I do not think it would be as big a deal if had pledged this semester in some ways I like it but in a lot of others I really do not it surprising to hear blake pledged I do not know if he wants to be friends  friends I wish had some I Am getting to know more people like in my study group but everyone here seems to have their own agenda its harder making friends than I thought I thought more people would be looking to make friends the guys next door are in a fight I bet its about that time when pot luck people decide if they are going to get along I do not like my roommate but at least I already found someon',\n",
       "  'n I hope its not raining right now or when I am ready to  leave My hair will frizz so bad and Ill have to blowdry it out to make it smooth again I wish I could where my hair naturally curly but its just too frizzy My niece has the best hair and her mother does not take care of it What are doors to side of this room for   Where do they lead  My hands are cold My feet are cold too I wonder if this shirt makes me look fat The stripes are going horizontal I need to start back to exercising like I used to That boy is kind of cute who just walked by My finger are really cold The air conditioning must be up pretty high in here I am always colder than most people Maybe I have small blood vessels in my hands and feet or something I just cannot get over how Princess Diana died She seemed to  be such nice person What is this black cord wrapped around the bottom of the monitor  What is it used for  I am sleepy and I have another class to go to today I hope I perk up I e never seen a window button on a computer before but then I Have never used a Dell computer before either How wide are the margins on this sheet I Am try to keep my typing neat I always like things to be neat and organized My mind is blank again I wonder what my mom is cooking',\n",
       "  ' I started to cry out of sheer joy at what he was saying to me I have never had any one person ever express their feelings to me and I do not think I have ever experienced love at this level I was so happy that the tears just came Oh to explain the next thing I want you to know that we have talked hypothetically about marriage several times in the past but it was usually never in a fully serious tone only half serious but each of us was contemplating it When he saw me crying he embraced me so tightly and with such warmth I felt at totally peace at that moment he whispered and I still do not know if I was meant to hear t or not that he had finally found his wife he pulled back from the embrace looked so deep into my eyes that it felt as if he touched my soul and right then and there asked me to be his wife I was soo taken aback that I could not speak I have never been proposed to and was not sure how to respond All I could do was smile one of the biggest smiles I have ever made and I replied yes He said that he was completely serious no hypothetical at all he also said that he would ask me again but this time would present me with a ring I know that it could be a while before he can get up the money for that but I am willing to wait for a man like that I have never meat anyone quite like him I have only been in love 2 times before one lasted for 3 years but it ended as an abusive relationship the second was only a one sided love He was leaving for the Marines and did not want to return the love because he knew that he would come out a different person But both times cannot even compare to my feelings for Terrence Now I come to my contemplations I had plenty of time to think about my life and where it was going on the bus ride home I know that I want to spend my life with Terrence he and I share the same values we are both religious people in our own ways we may not go to church regularly but we are both at peace with God and what He asks of us I have never met anyone that was religious and not afraid to admit it but he is not overly religious to the point that I am disgusted O  We both have the same values regarding marriage in effect neither of us believes in divorce Marriage is for all time and if you marry a person then it is forever til death do you part  I know that this could work The biggest obstacle that I can see is the age gap he is 28 I am 18 It has not posed a problem for us before my mother likes him and sees that he shows acceptable behavior and treats me with the utmost respect He never belittles me or disregards what I say because I am young In fact he looks up to me because I have set such high goals for myself and will stop at nothing to achieve them He also knows that I want t',\n",
       "  'ning out of things to talk about I Have found that happens to me a lot in conversation Not a very interesting person I guess Well I do not know Its something I Am working on I Am in a class now that might help The phone just rang Should I get it  The guy playing the guitar answered it for me Its for my roommate My suitemate just came in and started reading this I Am uncomfortable with that Hes in the bathroom now You know this is a really boring piece of literature I never realized how dull most everyday thoughts are Then again when you keep your mind constantly moving like this there is not really time to stop and think deeply about things I wonder how long this is going to be I think its been about ten minutes now Only my second line How sad Well not really considering how long these lines are Anyway I wonder what I Am going to do the rest of the night I guess there is always homework to do I guess well see This seat is uncomfortable My back sort of hurts I think I Am going to have arthritis when I get older I always thought that I would not like to grow ol',\n",
       "  'oommate but an empty water bottle inside a drink cup it kind of reminds me of abstract art like when homer tried to build a barbeque and ended up with a big pile of bricks with an umbrella sticking out I wonder if the McLaren really is the fastest road car in the world I Am pretty fast I think I could beat it Al pacino looks a little up set its ok though he is about to snort a whole big pile of cocaine what a crazy guy all this typing is making me thirsty again Bottled water is so stupid Why would you pay for water that somebody else just filled up out of a tap in the backroom when you can get a cup for free some people are just idiots People really do look funny when they cross the finish line in races It really is a lot harder then it seems to put the top on a bottle I wonder if I will ever use this stapler that I brought It does make my desk look very professional though I ate way too much pizza rolls fettuccine alfredo and chicken they all really do not mix A blue viper is cool My hair is getting long again if my dad sees me he will be like so son when do you want me to s',\n",
       "  'l eat it Why I miss home cooking not that my mother ever cooks but McDonalds serves as a home cooked meal for me since that is what we had for dinner a lot when I was growing up McDonalds is good I like the nuggets barbecue sauce is the bomb one time I went to McDonalds and they would not give me any barbecue sauce bastards anyway what was I talking about so I miss my home I miss my friends I called each of them the first week I was here they all stayed at home to go to college there or joined the military or just did not go to college Oh well their loss because its fun up here actually its fun in san antonio where I live I would probably not be going through anything like I am here if I were to stay home I needed this in my life to experience new things and new people funny though there are not that many Mexicans here at UT I mean there are tons of Asians blacks Indians and of course white people but there are not that many Mexicans so I guess there are people other than Mexicans in the world because in san antonio that is all you saw Mexicans man my friend jason is so cool I miss my friends from work I used to work at cicis pizza the best pizza value anywhere buffet for only 2 99 you come and see us  I always plug the restaurant to whomever I speak or whatever anyway I do not know why because the food really sucked yeah it was bad really bad rats and roaches that is all that comes to mind when I think of cicis yeah and pastries everyone that worked there threw parties they were the bomb I went to so many when I worked there anything and everything went at a cicis crew party although others were invited too they were fun I miss people at work I remember dating this grill I worked with never date anyone you work with people talk and shit happens and it really sucks frustration anger anyway that reminds me of my exgirlfriend now she makes me cring'],\n",
       " 'input_ids': tensor([[ 101, 1051, 5665,  ...,    0,    0,    0],\n",
       "         [ 101, 6881, 3637,  ...,    0,    0,    0],\n",
       "         [ 101, 2028, 2842,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 9152, 3070,  ...,    0,    0,    0],\n",
       "         [ 101, 1051, 5358,  ...,    0,    0,    0],\n",
       "         [ 101, 1048, 4521,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([[0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0.],\n",
       "         [1., 0., 1., 0., 0.],\n",
       "         [1., 0., 0., 1., 0.],\n",
       "         [0., 1., 0., 1., 0.],\n",
       "         [0., 1., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 1.],\n",
       "         [1., 0., 0., 1., 1.],\n",
       "         [0., 1., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [1., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = transformers.AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss combines a Sigmoid layer and the BCELoss in one single class. \n",
    "# This version is more numerically stable than using a plain Sigmoid followed \n",
    "# by a BCELoss as, by combining the operations into one layer, \n",
    "# we take advantage of the log-sum-exp trick for numerical stability.\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model for one epoch\n",
    "def train_model(training_loader, model, optimizer):\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    for batch_ix, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids) # (batch,predict)=(32,8)\n",
    "        ids = ids.cpu().detach().numpy()\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs==targets)\n",
    "        num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        #loop.set_description(f\"\")\n",
    "        #loop.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(validation_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for btch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7588b029d7a2416d8d5fe7fc9a002aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6970, val_loss=0.6910 train_acc=0.5152, val_acc=0.5348\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef477c33fef4663bbe23a310979ad28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6893, val_loss=0.6855 train_acc=0.5361, val_acc=0.5612\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e7f6490a6a4b79a4f37564487a9686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6783, val_loss=0.6876 train_acc=0.5700, val_acc=0.5553\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8879def7511c40e48b649b185afeae7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6617, val_loss=0.6826 train_acc=0.6023, val_acc=0.5526\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad6a81d5f9c4555a8a46f288ef08188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6372, val_loss=0.6845 train_acc=0.6401, val_acc=0.5606\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dab12e472764bb8ad6113b58606ee47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6013, val_loss=0.6940 train_acc=0.6783, val_acc=0.5639\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa5305471c74af9906063ac94c27b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5614, val_loss=0.7184 train_acc=0.7275, val_acc=0.5698\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb4830a0fb64728b540a47945d5bbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5163, val_loss=0.7535 train_acc=0.7611, val_acc=0.5542\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc68c8d823c4a3082c524ea9b185252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4712, val_loss=0.7689 train_acc=0.7953, val_acc=0.5482\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4523f9077cea4bdd9749d3adee02b49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4213, val_loss=0.8207 train_acc=0.8284, val_acc=0.5461\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    # save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(),\"ckpts/MLTC_model_state.bin\")\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "model = BERTClass()\n",
    "model.load_state_dict(torch.load(\"../checkpoints/MLTC_bert_model_state.bin\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5827027027027027, 0.6750079989433289)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_acc, test_loss = eval_model(test_data_loader, model, optimizer)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model,data_loader):\n",
    "    model.eval()\n",
    "    text=[]\n",
    "    predictions = []\n",
    "    predictions_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            text.extend(data['text'])\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            predictions.extend(outputs.cpu().detach().round())\n",
    "            real_values.extend(targets.cpu().detach())\n",
    "            predictions_probs.extend(outputs.cpu().detach())      \n",
    "    predictions = torch.stack(predictions)\n",
    "    predictions_probs = torch.stack(predictions_probs)\n",
    "    real_values = torch.stack(real_values)\n",
    "    return text,predictions,predictions_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test data\n",
    "text, predictions, prediction_probs, real_values = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: torch.Size([370, 5]), real values shape: torch.Size([370, 5]), text length: 370, prediction_probs shape: torch.Size([370, 5])\n"
     ]
    }
   ],
   "source": [
    "# print size and shapes\n",
    "print(f\"predictions shape: {predictions.shape}, real values shape: {real_values.shape}, text length: {len(text)}, prediction_probs shape: {prediction_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cEXT       0.50      0.81      0.62       178\n",
      "        cNEU       0.55      0.85      0.67       190\n",
      "        cAGR       0.58      0.60      0.59       195\n",
      "        cCON       0.61      0.59      0.60       186\n",
      "        cOPN       0.64      0.78      0.70       190\n",
      "\n",
      "   micro avg       0.57      0.72      0.64       939\n",
      "   macro avg       0.58      0.73      0.64       939\n",
      "weighted avg       0.58      0.72      0.64       939\n",
      " samples avg       0.58      0.71      0.60       939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omarahmed/Desktop/ML/Exploring-Explainable-NLP/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(real_values, predictions, target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
