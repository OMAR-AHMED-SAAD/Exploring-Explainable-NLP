{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 29 15:56:13 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   33C    P8     6W / 180W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model\n",
    "- Roberta Base Model for end part of text\n",
    "- Bert Base Uncased Model for start part of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model name\n",
    "PRE_TRAINED_MODEL_NAME_1 = 'roberta-base'\n",
    "PRE_TRAINED_MODEL_NAME_2 ='bert-base-uncased'\n",
    "\n",
    "# setting the dataset\n",
    "dataset='essays_cleaned.csv'\n",
    "\n",
    "# setting the data path\n",
    "if os.path.exists(f'/datasets/essays/{dataset}'):\n",
    "    DATAPATH=f'/datasets/essays/{dataset}'\n",
    "else:\n",
    "    DATAPATH=f'../data/{dataset}'\n",
    "\n",
    "# setting the checkpoint path \n",
    "if os.path.exists('ckpts'):\n",
    "    CHECKPOINTPATH = 'ckpts/Persnality_trait'\n",
    "else:\n",
    "    CHECKPOINTPATH = '../checkpoints/Persnality_traits'\n",
    "\n",
    "# training parameters\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid function\n",
    "\n",
    "# TOKENIZER\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME_1)\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME_2)\n",
    "\n",
    "# setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/datasets/essays/essays_cleaned.csv', 'ckpts/Persnality_trait')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAPATH,CHECKPOINTPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(DATAPATH,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, df,target_list, max_len=MAX_LEN,tokenizer1=tokenizer_1, tokenizer2=tokenizer_2):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = list(df['TEXT'])\n",
    "        self.tokenizer1 = tokenizer1\n",
    "        self.tokenizer2=tokenizer2\n",
    "        self.targets = self.df[target_list].values\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        textRight=text\n",
    "        if(len(text)>self.max_len):\n",
    "            textRight=textRight[:self.max_len]\n",
    "        inputs1 = self.tokenizer1.encode_plus(\n",
    "            textRight,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "          \n",
    "        )\n",
    "        # bert tokenizer\n",
    "        inputs2 = self.tokenizer2.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids_roberta': inputs1['input_ids'].flatten(),\n",
    "            'attention_mask_roberta': inputs1['attention_mask'].flatten(),\n",
    "            'token_type_ids_roberta': inputs1['token_type_ids'].flatten(),\n",
    "            'input_ids_bert': inputs2['input_ids'].flatten(),\n",
    "            'attention_mask_bert': inputs2['attention_mask'].flatten(),\n",
    "            'token_type_ids_bert': inputs2['token_type_ids'].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target list: ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\n"
     ]
    }
   ],
   "source": [
    "# get the targets list\n",
    "target_list = list(df.columns[1:])\n",
    "print(f'Target list: {target_list}')\n",
    "# create the datasets\n",
    "dataset= EssayDataset(df,target_list)\n",
    "\n",
    "# split data\n",
    "train_size = int(0.8 * len(df))\n",
    "test_size = int(0.1 * len(df))\n",
    "val_size = len(df) - train_size - test_size\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "# create the dataloaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids shape: torch.Size([16, 512]) Attention mask shape: torch.Size([16, 512]) Token type ids shape: torch.Size([16, 512]) Targets shape: torch.Size([16, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['     Hey well I am really very nervous writing down all of my thoughts and feelings on this computer and not in my own personal journal Because atleast there I know I will be the only one reading it But I guess I ll start writing about what I really have on my mind My boyfriend and best friend just took off on a long truck ride back home which is about six and half hours away They Jeremiah and Lauren came to visit this weekend and even though they have only been gone for an hour and fifteen minutes I miss them like crazy Jeremiah means everything to me and I cannot stand the thought of him being five hundred miles away But then again I decided to come to Texas and not to Tech Right Well that is one thing that is bothering me the other thing eating away at me is that my new stepmother who is only four years older than me is causing some major problems back at home And on that note it really ticks me off that as soon as I left home she has been on a rampage and really screwing up everything I feel really helpless because I am over here in Austin and I cannot help my sister or my dad go through their problems I usually stuck up for them when I was home and now I just get to hear about them over the telephone Which is even more bad because they may or may not be telling me the whole story I know my dad can handle it but its my kid sister I Am worried about Jessica my step mom is such a pain She is very very immature for her age and I saw this coming the whole time I just wish my dad did so he does not have to be going thru this right now I really miss home I even miss my dog Homie I bet he feels like I abandoned him I was the only one who paid attention to him but I hope Emily is taking good care of him now I Have only been in school for two weeks and it feels like it should already be november Sounds pathetic right I Am wondering if this was the right decision I sure hope so I am a firm believer that God will never give me something that I could not handle by myself So I just have to take this with a grain of salt and leave it at that I just wish it was as easy as that Gosh now it has been one hour and twenty minutes since Jeremiah has left I keep looking at the clock like he is fixing to get here instead of him being on his way back home We have become closer ever since I moved It seems weird saying that but I really mean it We actually have conversations on the phone we talk on the internet and when we see each other it feels like it did when we first started going out four years ago It feels great I love him and I can honestly say that I will marry him one day Heck Id marry him tomorrow if he asked me He is my best friend and I can tell him anything He is smart and funny and really good looking Better yet he is perfect He has never cheated on me and always makes me feel like a queen Sometimes I Am not sure if I deserve him But I do love him and I always will It still feels like I Am writing in my diary although if I was I would not be as consious about typos and mispelled words I really like this assignment though it sure beats the chemistry homework I really should be doing right now I hate chemistry I hate homework But it has to be done right I often wonder what would happen if I just said to heck with it all and quit My family would freak They are so by the book If anyone was to go out of the norm their whole world collapse I hope I Am not like that to my kids I want to be different I want to make a difference not wait for someone to do it and then say hey I helped them get there I want to be the one to do something something special I just figure out what I just know that whatever it is Jeremiah is there with me when I do it You know what Its hard to type when your nose is running But I Am almost out of time so Ill keep on typing I can do it I can do it Ha ha that is kind of funny Speaking of funny is not if funny how when your onthe internet you mispell words on purpose and here I am trying really hard not to misspell anything even when it does not matter if the words are mispelled or not That is bizarre Remember that song   How Bizarre   I hated it but Jeremiah always sang it and it drove me crazy That Is funny ',\n",
       "  'I just got of the phone with my best friend Lauren from home Potomac Md and it was weird because she has not left for school yet  she is going to the University of Pennsylvania  so When I talk to her we are coming from such different perspectives right now and it is really strange since we have always been so similar in everything before We were talking about our other good friend Meryl who is going to Cornell and has been there for about the same amount of time that I have been here and she was saying how she has been talking to her everyday but that Meryl is so happy and has so many friends and even a new boyfriend and I was saying how that is really weird and annoying because I honestly am pretty happy and having what I think is a fairly normal adjustment to college but I do not have like a real group of friends yet or anything and it is not everything that I had hoped college would be yet but I figure that I have only been here for two weeks Lauren was saying that what she thinks is so weird about it though is that Meryl says that she goes out everynight and loves school and everything but she calls her everyday and I say that I am still adjusting etc and I do not even have time to talk to her everyday I do not know I just think that it is really annoying and try not to talk to Meryl too much because it makes me feel like there is something wrong with me since things are not going quite as well for me and it makes me feel sort of bitter towards her Tonight I went out to dinner with my friend Katie who I met when I went on Camp Texas and I love We went to the Hula Hut and it was really good because I had really been away from Campus and the whole UT thing since I got here and I was sort of starting to feel like  I was stuck in a bubble and out of touch with reality so it was good to get away for a little bit Plus the food was really good and I had not had a real good meal since I got here either It also made me more sure that I had made the right decision about going home for a the weekend of Sept 1720 for the jewish holiday because I saw that it was good to be in car and get away and realized that it would definately be good for me to get away for a longer period of time Before I went to dinner tonight I was worried that I would not want to go because I would feel like I was missing things here but now I see that I definately did make the right decision Also when I go home I am going to drive up to Penn for the night to see Lauren and when I was talking to her it sounded like she was really excited to see me so that is good too I feel like I should be really stressed right now about schoolwork and everything with this being my first semester of college and my first time at a big public school since I attended a small private high school but I honestly feel like I have less work then I did in high school and that it is easier  plus I am really staying on top of it The only concern that I am having at all is the size of my classes all of them have over 200 people in them because I am used to being in classes with about 15 people in them and recieving a lot of focused attention from my teachers and although I am not concerned about this affecting my performance as I feel that I am completely capable on my own I am worried I will be overlooked and just be a number in crowd  which I Am sure that I will be in a lot of my classes at least for a while So far though I like my classes at least and find them interesting and all of my professors do seem to be pretty interesting and approachable I also rushed and am pledging a sorority which even though I really like it and am happy that I did it is becoming more and more time consuming and I getting worried that I am going to have trouble completing everything that I need to do for that but I am sure that it will not be as tough as I am expecting Right now I am listening to a really good CD that Lauren sent me that our friend Dave made for me and it is so good because it is a mix that he made of all of these really good old songs plus a couple of good new ones I have three minutes left to write now and I am really running out of things to say except that earlier I found a cricket or grasshopper or something in my room and it is really gross and I do not know how it got up here because I like on the 11th floor of dobie and there are no windows but it is totally gross and I am definately going to file a complaint and tell the front desk that I need an exterminator or something Well I think that my time is up now so bye ',\n",
       "  'Gone with the wind I am and away I look out into the empty space of eternity with nothing I awake into the morning darkness hearing the buzzing noises and look at thee General Electric alarm and press snooze while i squeeze in 1 hour of extra sleep I watch tv and walk the whole day and then i go shit in my public restroom with feces flowing from my colon down to the toilet Oh yeah i say as I struggle I often have a tough time No i do not I do not know why I am saying that I brush my teeth and develop anger and excitement at the same time Anger for what i ask to me as i am me but do not know who you are Who are you and why do you exist i say Then i look out the window and see big yellow neon lights expressing Wells Fargo I take a shower with no showerhead pressure and I get pissed off I want nice warm water and a nice showerhead The force is strong as I see red marks become apparent on my not so hairy chest I get the urge for herbal essence but see to see that I do not have any Do you have the urge to say   oh yes   I am away and out of the blue and in the light of things there is nothing Darkness exists as a prerequisite of life What is this   doh   Homer Simpson says Donuts and coffee i must have soon Soon enough to satisfy my craving for IHOP pancakes and a nice morning breakfeast at a nice restaurant I go down the elevator Stupid ignorant people who live on the first five floors are too lazy to walk down the stairs I had a firedrill today How annoying the pulsating noise was And how tiring I could not imagine how firefighters climbed up to 78 floors on that clear but smoggy morning on September 11 2001 My love for skyscrapers has grown more and more Everyday my passion and desire to see new and taller buildings arise grows larger and larger I wait to see what will be done with the hollowed groundzero Perhaps nothing will be built but i hope for something Tennis is a very vigorous sport The agility and the swift hits of the nice yellow ball that i throw around to my dog with Oh I love my dog Nice gold fur blows in the wind as i see a slow motion love seen in the mix of progress and then he starts to hump me Humpty dumpty sat on a wall but then he fell off I forgot how the riddle goes The itsy bitsy spider climbed up to window is it Who cares Not me for I am no longer in kindergarten but in college the USAs largest collegeUT WhoohooO I have the urge to get drunk perhaps maybe have fucking good sex Perhaps smoke something illegal or not My roommate has just farted and he says sorry I do not care The smell is of essence Nice allure should be made into such a fragrance and entitled El Natura Body works in different ways My stomach is hurting I have been having a bloating stomach the whole day It is like a balloon filled not with oxygen but with helium If i could only float in the air and swim in the sea Fly up in the air Perhaps the ultimate luxury would be to eat tabasco flavored cheezeits when I am perhaps   high   in the sky Oh yeah good old times rock on forever Forever i must live like that Drunken little bastard i must be during this year Ambitions thrive to do well but will I Parents constantly pester me with Do well in class vijubaba Please do well Of course the usual response is the usual for almost everyone else  I will   comes out of my mouth Deep inside i have the heart to do whatever I want but it is the concentration that I lack I do not like being forced to do something that I do not agree with It is simply at its upright most stupidity Stupid is as stupid can be but who is stupider than stupid Everyone is judged at different levels Ignorance is bliss honey Bees fly around I have never been stung by a bee or wasp Speaking of being stung I like the singer Sting He is a good singer Anyways speaking of other things i think things are the way that things are and that things should be left to the way that things should be WHY because they are things that obey other things not stuff Yes I need to pee but must I Flaming whopper commercials thrive in the background of my head as i feel the urge for el penetrado Oh yeah The music is playing and I must go now Bye bye and bye         ',\n",
       "  'I am thinking about my roommate who got the wrong book for his Japanese class and did not realize it till he tried to do his homework last night he had to get up this morning and drive to get the book and try to do the four pages in the book that were assigned for homework before class started He is crazy he and I always get into trouble like that We Have known each other since elementary school and have had some great times One time I remember we were studying for a chemistry test our sophomore year in high school it was really late so we decided to go to the store and get some jolt cola to help us stay awake I ended up backing my truck into the ditch in front of the store and we were stuck We sat there forever trying to get my truck out of the ditch and then finally I called another one of my friends who had a four runner He took us to WalMart and we bought a tow strap We  went back to the ditch and hooked our two cars up together He then tried to pull my truck out and his tires just spun around and around So I had to accelerate and my roommate had to push my truck from behind by the time we got my truck out of the ditch it was like 2 so we were way to tired to study so we  went to bed and failed the chemistry test I ended getting a 79 4 in chemistry and being moved to the academic class because of 110 of a point If it was for  my roommate coming up with the great idea of going to get jolt cola in the middle of the night I might have been able to stay in chemistry honors but even if that was the case I would much rather have that memory than an extra point on my high school gpa My roommate and I now live in a two story townhouse that is 1180 square feet We got a really good deal on it because the week before school started the apartment complex we preleased with called and said that had no apartments left for us even though we preleased 4 months ago We had to drive to Austin and look for  a new place that weekend and while we were here we stopped by the leasing office and talked to the people there they said that someone cancelled on a townhouse and that since we had to go through all this trouble we could have it for almost the same price we were going to pay for and apartment about 34 the size We got so excited and I am really glad things worked out like they did because I do not think I could live in a small apartment because I need  a lot of space or I go crazy too much space makes me crazy too but only when there is no one to share it with But our townhouse is just the right size and its nicely furnished because both of my parents have been  divorced and remarried so there is a lot of furniture that no one uses so I got some pretty nice stuff There are not many decorations in the townhouse but there are lots of dishes and silverware I took my stepmoms couches which really surprised me because I did not think that she would let me have them because sometimes she is strange but I guess she can be pretty understanding sometimes I used to hate her but now I realize that she really does want the best for me and only gets frustrated because she raised her kids differently than my dad raised me ',\n",
       "  'blue is a pretty color this is a very simple website i need to work on my website i need to finish it i never finish a wesite when i start it why cannot i finish anything  am i a failure  my parents are really proud of me i love my parents i do not want to let them down i miss them i want to go home i do not like being away from them september 11th really scares me i was so worried on that day i can totally remember where i was that day i was in weinbergs class i remember how he reacted that day i went to history to get a sense of comfort i did not find it i do not want to go to war war scares me why am i always so tired 20 minutes is a long time i am very glad i have already finished my experiment requirement for class those surveys were really boring i wonder how i am doing compared to other students in my class i want to read my email i need to do my astrophysics homework i am bothered by my lack of selfcontrol in doing my homework my finger hurts i am really sick right now i think i may have the flu i wonder why my head hurts so much i really want to read my email why does my neck hurt  i want to take a nap i wonder how much i should type is this enough  my arm is hurting now i want some snacks i need to quit eating so much i need to lose weight i miss my granny why did jo have to scare her  she knew how much it would upset her why did she have to cause her to die  i want granny back i really miss her i made a fool of myself at the funeral why cannot i deal with this and quit obsession over it  was she proud of me  did i make her proud  i loved her so much my fingernails are dirty i need to take a shower i am almost halfway done i know so much yet i do not know a thing why is the smithsonian preserving pieces of the world trade center  should not we leave it alone  i love kara i cannot wait until i get to wake up with her everyday she makes me so happy i hope i make her as happy as she makes me i just could not handle it if she did not love me time is slowing down i want to call kara i want to see how she is doing on her writing assignment i think human beings are extraordinary how can we deal with so much  we deal with physical adversity as well as mental my fingernail looks weird its been a nice day my skin is peeling i want to play racquetball wow its already 525 weird it seems like i have been here for an hour i want to be more physically fit i want to please myself and kara more i want to be around for a long life i am tired of being fat i want to help myself i just hope i can i want to daydream i want to be in star trek i want to have those kinds of computers and technology i want to be a great commander i want people to be liked by many i want people to want to be me i want to be someone that is liked and respected i am very insecure and have low selfesteem why do some chemicals hurt people  i mean why cannot we find some way to neutralize them  i am done         ',\n",
       "  'Okay I am writing this thing called stream of consciousness writing I find it hard to type right now I do not know if I should follow proper grammar My Norton firewall just detected another blocked intrusion I do not like this Annoying hackers Why do not they just go one with their lives without bothering decent people like me I have 4 roommates and they are all good except for one He is nice and all but he does have annoying habits and it seems he does not know proper manners He watches TV loudly even though he knows people are trying to sleep He even woke me up listening to music using my own computer without asking my permission to use it That is so annoying and I cannot stand it right now My goodness it has only been 3 mins I Have been writing a lot of things already My arms are hurting because I cannot type for so long and I hate typing I Am not a typer my brother and cousin are flawless typers but I am not I already had so many errors in typing right now it is so annoying me I want to destroy this key board I want to use the voice typing method but even that is a pathetic technology which is still in its infancy This is really annoying me I want to take the psych research right now but I cannot because I am not 18 yet That is a load of bullshit if you ask me cannot it be 17 What is your advantage to your peers in the same batch or class who are older than you by a few months A few months of extra knowledge I do not think so I hate my birthday there is also a math midterm on my birthday hooray for me how nice what a nice birthday gift I have a tendency to measure how much I type I always limit myself and not going to much bec I always fear my efforts are being wasted or something and I am very lazy so I will rather do nothing or stop doing something if there is no punishment involved Like this writing assignment right now if this was not graded or the lose of not doing this is very minimal I will just stop and just receive the consequences rather than waste 20 minutes of my time doing nothing but write senseless writing And I ate writing I hate the damn typo errors I hate now typing fluidly like others I hate making mistakes I hate typing because it hurts my hands and fingers I hate this activity I am not in a very good mood right now I kind of understand that I wonder why Is it my roommate maybe is it this senseless activity maybe maybe its because University of Texas has cut my freaking internet no I cannot download beyond 8 gigs nooo bec you are taking up too much bandwidth and leaving none for the others What do I say I say to hell with the others what do they care if they do not really use the internet in the first place They are just wasting bandwidth on nothing This is just pathetic You guys are afraid of the recording companies the RCAA or something like that That Is what you are Afraid Why do not you just leave us alone like other schools They have FREE internet not this cheap limited you have to pay for internet I am just ranting away all my frustrations on this thing are not I I hope no one reads it if you do the hell do I care Damn it College life sucks To much reading Everyone expects you to read all the chapters and expect you to understand the next day during the lecture Well tough luck we are not that smart Some people do well in lectures and some do well with books Some people just can understand with the book so why do you pick on us Everything you test should be on your lectures The book is just for bringing your knowledge of the subject to a more concrete manner Lectures are what really is important and you just read the book to review You do not test what you do not teach in lectures Thank God I only have a few more minutes left My fingers are burning now I cannot be stuck doing the same thing for prolonged periods of time It is just not right It is so boring Funny though if you come to think of it Ironic I cannot stand doing a same thing for so long but I can stand doing nothing for so long What kind of person am I Indolent Lazy Pathetic That Is what I am A person who does not want to leave his little protective sphere a controlled sphere I am afraid to do things with risks I will never succeed in life like that Doomed not to be a leader ',\n",
       "  'okay I hope this works because I already did this assignment but some one called in and took me off line and erased my entire message ooo frustration and things have not been going exactly right lately anyway Ooo this key board is really dirty I should clean it should what a funny word I should do lots of things Sunday morning man it is hot my ac was out for the last two days but it is on now so this world is alot nicer I am in a better mood I feel relaxed this morning peaceful Sundays seem to do that to me especially when the Monday is going to be a holiday The fan is blowing on me and I feel great I ran this morning 8 miles around town lake and it has to be one of the hottest mornings yet wow very sticky I felt exhausted and sick when I was done but nothing water could not fix The radio is on in my room the TV in the living room and my roommates radio in her room many a noises going through my head Oh I need to change my calendar page tomorrow I just noticed that august is over wow that went fast each year seems to go faster then the last I can remember being 16 thinking college would never come and now I am a junior and college is almost over what happened dishes I need to do the dishes before I leave today I am going out to my ranch to meet up with my family for the day Why is there chocolate in my bed That is odd The arch of my foot hurts no good cannot have that for running tomorrow there is a 4 mile race around town lake 730 am maybe Ill run in that maybe Ill actually sleep in one day I have not yet not really since last spring semester It was the busiest summer that I have had in a long time poof Oh I need to write Andy a letter I keep forgetting It was nice to talk with him yesterday especially since he paid the bill That always helps I have quite the movie collection going now I just noticed all my cassettes I do not like this song playing now hey the TV went off I do not want to drive today I am tired of driving My relaxed attitude seems to be disappearing I am beginning to think of all the readings I have to begin doing I took 15 hours this summer so I am tired what happened to that thing called break I think it forgot me but I had a great summer cannot complain plus now I am that much more along I am glad I did what I did The list for WalMart just was delivered to me by my roommate hmmm what do I need or want diet dr pepper I have to have that stuff yum great for the caffeine well the day should be one of reading and talking I love the weekend life Talk again with you later Good bye ',\n",
       "  'I am sitting here at Hardin House boy I am so stressed out I have these  two writing assignments for psychology a whole bunch of precal homework  and tons of reading to do for Biology and Sociology this sorority stuff  is starting to take up way to much time I really do not like the feeling I  am getting when I am set up with a date because it makes me feel like I am cheating on my boyfriend I know I should not feel bad since I am just being friends with them and nothing more it just really bothers me sometimes I really do not feel like going out tonight because I still  think I am sick from yesterday I need to go and call my mom and dad  since I have not talked to them in about five days I am just way to busy I think I really need to work on some major time management skills because I  am worried about getting behind and I want to get 3 As and 2 Bs so I can  start off good before my classes get too hard I am really excited about  UT vs OU weekend I hope that Jaycob can go so that I do not have to go with some random guy I wish it was not the same weekend as my schools homecoming so that I could go home oh well I need to take a shower and get ready  for the KA mixer and try to read and definitely watch 90210 That was 20 Minutes THANK YOU  HAVE A NICE DAY ',\n",
       "  'It feels like more time has passed and that my life has been turned upside down Not too badly upside down but it is definitely different I had friends come home this weekend and I loved seeing them but I also realized that college is different for all of us I noticed changes in my friends that did not affect our friendships but nonetheless were there I loved having friends home knowing I had people to go out with and overall just having a blast But it just reinforced the distance between us once they left I love college and all the new people I Have met I just have not had the time yet to make the kind of friends I had in high school I understand this but its frustrating to go from always having 10 people to call and talk to down to maybe 4 or 5 I met some people at the Longhorn game on Saturday and it was awesome  I just hope that that will keep occurring and soon enough I will have those close friendships with people in college My friends and I also talked for quite a while last night about one of our friends who went to West Point I Have realized that I miss him but at the same time have mixed feelings about that sense of loss In so many ways I wish he were here but I know that would not be right It does not make it any easier just a little bit more complicated I Am also trying to get over my frustration of living at home this year I feel like out of 50000 students I am the only one staying at home with my parents In so many ways its not too bad but when I go out with friends and they can do whatever come home whenever and so on it makes me realize how much freedom I Am missing out on A lot of it is that I am sad that I do not have the advantage of meeting people in a dorm and making new friends that way Its also hard because so many of the people I am friends with are all living on campus and so its a lot easier for them to get together than it is for me to get together with them Anytime I have to come up to campus its at least 20 minutes and a hard time finding a parking place It just sucks because that is really the only thing I do not like about my college experience so far It always makes me laugh when they talk about the freedom from your parents that college brings because I do not have that This is an awful long time to write I do not really know what else I have floating around in my head and I still have 10 minutes School work is starting to kind of scare me Its like oh I have a test in 3 weeks so I do not need to worry about it now and then I know that all of a sudden 3 weeks are going to have passed and I Am not going to be prepared I love not going to all of my classes everyday It makes it seem less difficult The one thing that really sucks about my schedule is the fact that I have these huge breaks between classes On Wednesdays and Mondays I have 3 hours between classes and 1 5 between 2 others The worst part is that I do not know what to do with the time and I cannot go to my dorm because I do not have one I Am starting to hope that the next writing assignment is not to explain your feelings about college because that is what I Have done in this one so I might be stuck at the computer for awhile next time I Am listening to the Titanic soundtrack right now I Have been listening to it a lot lately I think its because its pretty relaxing and it calms me down The music is beautiful and it makes me think of the movie I went to see Titanic twice The first time the movie cut off in the last part of the movie It totally ruined the effect and it really sucked because they turned all the lights on and well you can just imagine 5 more minutes I Am really glad that I did not have to work today I feel sorry for the people who did have to go up to work on our first school holiday I work up at the recreation center on campus and I love it The people there are really cool and now there are some people who are my age This summer when I worked there there was no one under the age of 21 except for me It made it kind of weird because they were all in a stage of life that I had no idea about and will not for at least another 3 years One of my friends from high school works there and I like having her there I hate the fact that we do not work together but hey that is the way life goes I wish I had more hours simply because that would be something for me to do with my down time on campus Two more minutes I never realized how long 20 minutes could be I Have found email to be quite handy with friends because I can email my side of a conversation in about 5 minutes instead of having to go through the pain of writing a letter putting it into the mail and sending it I have no mailbox so it makes it even more difficult to mail a letter because I have to go all the way to the post office Whoever thought of neighborhood community mailboxes with no drop box for outgoing mail needs to be laid off of his or her job Well looks like my time here is done I guess I have one more to do but not until tomorrow ',\n",
       "  'As I sit here and drink this very nasty coffee coffee maker is on the fritz I look back and recollect on this past weekend In fact this has been my only thought since Monday morning So I can give you can better understand what I am talking about I will give you  a background premise to what occurred I was sitting on the computer as always talking to my boyfriend in Houston on a chat group He and I are still in that newlywed phase of a relationship so we are constantly wanting to be around the other My having to be a couple hundred miles away in school is causing major distance pains Any way I was trying to encourage him to come visit me for the weekend He was regretfully working on both Saturday and Sunday lunches  he is a waiter where we met this past summer  He replied to my requests with a request for me to go down there I thought about it for a brief moment decided that I had no major plans for the weekend and immediately called Greyhound I was on the next bus home in one hour He was so completely surprised As he put it it was one of the largest signs of love that he had ever been shown for someone to just up and leave on the spur of a moment spend over 50 that he knew I did not have to spend sacrifice a weekend of my time just to see him He was on cloud nine and as soon as I saw him so was I Well the weekend progressed and I had a wonderful time I also had a chance to meet up with my friends still in Houston without having to report in to my mother thankfully I neglected to tell her that I was in town Well on my last day there before my ride to the bus station he and I had a heartfelt conversation He basically proclaimed his undying love for me and told me exactly how special I was in his life He also went over all the things that I did to let him know how much he knew that I love him He remembered all the little things that I have done since day one which really meant something to me because I know that my actions are received with such appreciation He felt that this last one my coming to see him on the spur of a moment was the ultimate He then proceeded to tell me of a mental list that he had of who he envisioned his wife would be what kind of person she would be and what her traits would be He told me that I fit every one and he said that I was his perfect companion Both of us have seen a lot and have dated almost every type of person out there I know that he is the one for me I have known it for a while I have been told that when you meet that certain someone you will know it is them and you will know that they are the right one for you Well my mind heart and soul are screaming at me that Terrence is him Well I stated a few things to him that I had been thinking about and at one point  I started to cry out of sheer joy at what he was saying to me I have never had any one person ever express their feelings to me and I do not think I have ever experienced love at this level I was so happy that the tears just came Oh to explain the next thing I want you to know that we have talked hypothetically about marriage several times in the past but it was usually never in a fully serious tone only half serious but each of us was contemplating it When he saw me crying he embraced me so tightly and with such warmth I felt at totally peace at that moment he whispered and I still do not know if I was meant to hear t or not that he had finally found his wife he pulled back from the embrace looked so deep into my eyes that it felt as if he touched my soul and right then and there asked me to be his wife I was soo taken aback that I could not speak I have never been proposed to and was not sure how to respond All I could do was smile one of the biggest smiles I have ever made and I replied yes He said that he was completely serious no hypothetical at all he also said that he would ask me again but this time would present me with a ring I know that it could be a while before he can get up the money for that but I am willing to wait for a man like that I have never meat anyone quite like him I have only been in love 2 times before one lasted for 3 years but it ended as an abusive relationship the second was only a one sided love He was leaving for the Marines and did not want to return the love because he knew that he would come out a different person But both times cannot even compare to my feelings for Terrence Now I come to my contemplations I had plenty of time to think about my life and where it was going on the bus ride home I know that I want to spend my life with Terrence he and I share the same values we are both religious people in our own ways we may not go to church regularly but we are both at peace with God and what He asks of us I have never met anyone that was religious and not afraid to admit it but he is not overly religious to the point that I am disgusted O  We both have the same values regarding marriage in effect neither of us believes in divorce Marriage is for all time and if you marry a person then it is forever til death do you part  I know that this could work The biggest obstacle that I can see is the age gap he is 28 I am 18 It has not posed a problem for us before my mother likes him and sees that he shows acceptable behavior and treats me with the utmost respect He never belittles me or disregards what I say because I am young In fact he looks up to me because I have set such high goals for myself and will stop at nothing to achieve them He also knows that I want to wait until graduation or at least close to it before I get married And he knows how important Medschool is to me He also says that he will not marry me until he knows for sure that he can support me he is one of those chivalrous guys that believes that a man should be able to support a family on his salary alone but if his wife wants to work or even makes more money than him then he is all for it he just wants to be able to know that I do not have to work At the same time he is giving me more support about being a doctor than any other person including my family  One thing that bothers me is that he is 28 and still a waiter I know that it is only temporary he is working for a career in music either with a band as a musical engineerproducer or as a teacher he is one of the best drummers I have heard and writes INCREDIBLE music everything from piano pieces to synthesized complete modern music he can do it all  I know that with my income alone we can make it even starting out being a doctor will be able to support us The problem could arise in the fact that we will be in debt for a while paying off medschool and it would be a shaky start But of course all marriages start out financially shaky I am also concerned that because he is so much older and more ready to settle down that he will encourage me to marry before I graduate that would just be another added expense By the time I graduate he will be 31 I just hope that he can wait that long he says that he can and that he will I believe him but he may grow antsy as the years go on  Bottom line is that I love him and will do anything in the world for him but of course within reason I also know that he would do the same for me without even needing me to ask I have never felt this way about any other human being and I do not think that another love like this is possible besides I do not want any one else but him soo Yes I will marry him and we will have beautiful children We know that once this honeymoon feelings wear off our love for each other will change but as I know my love I will still be there by his side I have never met anyone that completes me and who I am so perfectly as he does not even my best friend Terrence and I connect we realized this when we first met and had our first deep discussion In fact we connected to each other so well that it scared both of us and with reason when you meet someone and the next week you can complete each other sentences and already know what the other is thinking without them having to say a word it can be a bit scary But as we grew we grew more comfortable with the idea and now he is ready to marry me my life could not be more perfect and I could not be any more happier I am in love and now I am engaged ',\n",
       "  'I am happy that I made the transition from Georgia Tech to UT I am cautious about my future and what I want to get involved in I want to become a Texas Wrangler but I am unsure that they will take me I want to do well in school and get in to law school I do not study as much as I want to and I need to work on that I love sports and I like football most of all I wish I had played football was a kid I am excited about the future of our country and I want to get involved I do not understand what this assignment is about but I will write everything that is on my mind I worry a lot about stuff that is of no concern to me I try to be the best student and person that I can be I have a lot of pressure on me from my past and the way I have been raised I Have always been a good student and pretty athletic and I try to be the best at everything I knew that is bad but I feel that that is the way I was born I believe our President is guilty and should be impeached for the good of the nation He is a smart man but he is personally corrupt I have a roommate that is good He is Norwegian and is a great student He is fun to hang out with and we get along well I have a girlfriend and I Am not sure if we should stay together We get along well but I feel like I have the whole world at my fingertips and she is holding me down   WE share a lot together and we are best friends but she wants more more time more money etc My parents are strong Christians but I am not sure that that is what I want College is a time to figure all of this out and I am glad I am here It is a fun place and the knowledge here is overwhelming I am like a sponge trying to get it all in my head I want to be an attorney and I feel like al the knowledge will make me a better lawyer I want to be the best that I can be and make a good future for myself My Mom never went to college so she wants me to make the most of it It is a lot of pressure from her and I try to be a role model for my younger brother He is a trouble maker and I want to be a positive influence in his life He is a good kid but the friends he is with are not good for him I love my family very much I cannot wait for the UT UCLA football game I love the game and I love the school spirit it provides I was an engineering major at Georgia Tech last year and I made the switch to UT all by myself I am proud of that move and I would not have it any other way It was the best move I Have ever made I like the big school here and the number of students here I want to run for public office when I get older and make this country a better place I feel like I can be a good leader and bring good things to this country I am a good person and I believe others will think so too  ',\n",
       "  '     I wonder what this stuff is suposed to mean What can i possibly learn about my self by writing incohearant thoughts for twenty minutes I Am not a very good typist for one None too good at spelling either I hope who ever or what ever machine looks at this can read past any spelling and punctuation mistakes This computer screne is far too birght i wish i had thought to fix that before i started this I wish i could read some other students writtings Maybe id find some insight to how i am doing in this rat race Maybe id figure out what the girls around here are looking for how vacant can my mind be how many mental diseased is this going to uncover in me i hope there are drugs to fix it I wish i were at the lake right now no books no tv no phone just a hamock and a cold drink Maybe a seadoo A little bit of water sports never hurt anyone Accept the ones that crash God i hope  never see anything like that again Why did that have to happen all those kids had to do was go a little bit slower All they had to do was look around for boats They could have avoided it and now they are dead just beacause they could not understand that a jet ski is not a toy A goddamn toy how could anyone   play   with their life like that I hope i go in my sleep i can feel the propeller slicing through my leg right now those poor bastards i guess id rather be one of those kids than the poor bastard that hit them though i could never live with that kind of baggage they did not even know where their parents were for the love of God what could they possibly have thought when they got that phone call   MRs smith your daughters are dead There was nothing we could do i guess these things just happen   and how could the parents not blame the draver of that boat how coul anyone ever admit that their childs death was their childs own fault that all seem a bit bleek Maybe i am ill maybe i should get a bike I could get around alot faster Maybe i should just leave early then maybe i could loose some weight mayeb i have got it all wrong though maybe they do not even care about that I should loose some weight anyway its unhealthy Who ever reads this thinks i am a nutcase  Hello out there welcome to the inner sanctom of my mind its cold in here but you will get used to it watch your step there are some loose screws here and there do not poke at that i am affraid it might fall apart do not trip on that its allready fallen apart i know its messy in here but ill get it all organized sooner or later   i need to hire an inner sanctum of the mind cleaning lady I hear they are very affordable I guess cheap labor is just a byproduct of his whole immigration mess maybe that mess has to do with our economic mess or our terrorism mess or our youth mess or our drug mess who in hell is going to clean up any of these messes can you do one at a time or are they all linked together certainly some are ill just let trusty george W handle it err imean cheany  ',\n",
       "  '     How much time do I have before I need to do some more homework This week is going to be pretty busy It is hard to find out what is due and when because their are way to many different places to find out All the teachers should use the same service This would make it much easier for students to keep up My goal is not to fall behind but I have missed some classes of geology I need to find out what I missed That class is going to suck I wish I had a car so that I could get places farther than a mile away I hate writing I hate typing It is hard to write down everything you think Our minds work 5 or 6 times faster than one can type or speak So if we could talk faster we could get up to 5 times the amount of information to someone else leaving us with more free time This would shorten classes and anything that requires communicating I Am glad my spider bite went away It sucked that I had to go to the health center for it They gave me a shot in my ass It left a bruise but it did not hurt until an hour or so later It just sucked that it was on my face It made me nervous when I heard that corterzone lowers the immune system because I had been putting it on for many days Thank god the over the counter medicine does not do harm I spend much time alone now that I moved away Luckily I enjoy being by myself It is nice to have the peace and quiet The parties are fun it is just that I would prefer to chill with just my friends not a million people that I do not know I do not know too much about bull fighting most other sports I have played I do not see the glamour passion and joy of bullfighting It seems barbaric to use ones instincts  ',\n",
       "  'day long hot muggy water drips slowly out of the ground non sequiter my mother peaks out from beneath the trees and laughs leaves shaking from her hair a ground develops where there was just empty space before I need sunglasses not to be blinded chocolate bars scream nothing and jenny craig addicts eat them yummy yummy day is so hot and muggy the sky is falling chicken little must have been surprised she must have screamed wailed a thousands songs the fox picked them up when they bounced of my throat and stuck to the corners of the world stickier sticker than any glue any oatmeal face prepubescent girls use to give themselves a facial thought thoughts hard to track when they come so fast or not in any order whirling monsters in my head searching for some shape searching for the perimeters exit monsters under my bed no matter how big the rat trap was it never caught they never took my bait instead they curled up beside me until I was no longer afraid afraid until morning scared me more than night and night felt beautiful empty when you cannot see something anything is possible it is easier to dream the daylight defines things in ugly fluorescent light not even with a decent amount of shade in daylight you discover your mother is old and your grandmothers hands made of paper and once you realize you will become only dust and are not afraid then you know the comfort of end and you look forward to papery skin because it means the end will soon come of being so fucking tired and you can rest and then you become scared to think about heaven eternity who really wants to live forever endlessly Id scratch my eyes out with boredom then you become scared of your name because it fixates your identity and you are some how attached to it it you repeat your name enough it is dissolved into eternity and then you cannot get out of bed in the morning morning bring day day defines defines disceiting pea pods in father orgasms as the little miniature gypsies walk by laughing and singing troll songs charging me ten dollars to come under their bridge frogs frogs never turn into princess and if they do they immediately shoot themselves in the head there just is no going back ever ever when alice looked into the looking glass she just feel and feel and fell and landed on the mad hatter kettle it was her unbirthday happy unbirthday to you red lines flick across my head my tongue is not long enough to catch them no honey for bait I observe quietly this is how the universe was created splat spat humpty dumptys egg whites hit my head and Jill came stumbling after on one leg the jolly green giant has her other one locked in a glass jar on his table his children hump it before they go to be their daily confessions to their father father I Am sorry I have sinned please watch me do it repeatedly wierhoden Sie Bitte I was out of the room Wierholden Sie Bitte Danke Shon my bonnie lies over the ocean inside the ocean nothing crawls out on legs legs develop into fins connect the dots lalala surrealism is incited by a kid with a kite flying upside down dali looks at a patty melt sandwich and discovers a movement someone looks at the wrapper and defines ecology Kermit the fog pukes and pukes and pukes miss piggy makes him eat it up with a spoon a black rusty one saying O kermy or kermy mashed potatoes on the computer but not gravy cafeteria kind looking like an ice cream splat my friend used to always eat them with her finger while snot ran down her nose I was embarrassed for her and ashamed of my snobbery pee girl gets the belt outside the black ground decapitated head grew on trees a skull inside inside the most delicious porridge anyone had ever eaten stood bubbling bubbling no one ate except the very very very bad girl th3e girl who said nothing rocked herself under the tree growing silent roots if I say nothing no one will ever be offended it never occurred to her that her silence was offended like in the same way she discovered later as the smell of her cunt her own body was offensive defensive but she loved it once she learned how and all the kings horses could not break her again and all the kings horse never even tried she grew wings but tore them off over and over and over the white wall is turning blue the more I stare at it the closer it becomes I Am suspended by it women walk behind it wailing I cannot reach them I Am to tired to scream to tired to move my hands are creatures of their own highly adaptive spiders crawling over letter letter letter form words word are symbols of meaning but if you repeat the word mom over and over and over it makes as much sense as an animal sound mom mom quack quack mom mom quack quack absently no difference past a certain point the little girl told the grizzly bear she was lonely and scared the grizzly bear said so am I if that had been a brother grimms talk the grizzly bear would have taken of her head click click click or ticktock the mouse runs up the clock no matter what the nose pink walls and Pocahontas braids swirl in the trash spider are becoming tired and tired and my hand are empty again thank god ',\n",
       "  'My thoughts sensations and emotions are always changing with each new experience idea and whatever happens throughout my life They are affected sometimes by what other people think and sometimes affected from just maturing and thinking differently Right now I feel like I am in control of myself because I am doing well in school academically socially and mentally School work is very important to me and I make sure that I do well I do not go overboard like a lot of my friends who study hours every day Socially I have adapted very well in meeting new friends Mentally I am in control and focused on what is going on in my life There were some problems that have caused some stress in my life since college started First me and my roommate got in some petty arguments but now we are getting along just fine Also I had some guy problems I regretted doing some things I knew I should not have done but now we worked things out and or friendship is even better than before I know that I have not been eating right and sleeping enough which is causing some changes in my body that because me to be more tired and weak I always go to sleep way too late and wake up early for my classes Last night a friend called at two in the morning and I talked to him for hours even though I should have y eightoclock class I am in a very good mood today because everything seems to be working out for me I Am usually a nice pleasant person until I am faced with a lot of stress or go through PMS Then I get in a really bad mood My friends know when to stay away from me The only thing really troubling me right now is all the reading I have to do for my classes the assignments they give us are no problem but the reading is crazy They bombard us with reading assignments that  really are not necessary My art class for instance requires for the present moment for me to read from pages 13 to 200something I really do not want to read about all these statues and paintings and sculptures from different eras Anyway I miss my family but I Am not homesick at all How can I be when my parents call ALL the time  I know they care for me but its really not necessary for them to want to visit every weekend which I will not allow them to do That sounds mean but they are going to have to get used to it I do miss the free laundry and the homecooked meals and my own room and my nice bathroom but oh well                     ',\n",
       "  'Well I have procrastinated on doing this assignment for long enough and so I guess I should start working on it now Its a thursday afternoon and all I Am doing is staying at the apartment to do this assignment What a life These stupid popups keep on coming up its interrupting my thinking process Let Us see I Have been up here for about a month now and I have yet been able to see and fun that everyone that went here or is going here claim Its not that I want to be partying everyday still I would like to see what is all the fusss about Then again my life here at the apartment is great I live with two roommates each have their own eccentricity still without them I would be bored out of my mind These restaurants around here keep on posting flyers in front of my door its getting really annoying I Am hungry yet I do not want to go grab something to eat  does that make me lazy Of course not Its because I have to sit here for 20 minutes to write this assignment yea that is it During the day this place is so quiet almost everyone that is living here goes to UT so during the day they are at the campus Maybe I should spend more time at the campus Maybe that would help me get to know more people I have not made one single friend yet well besides those people that I see in class everyday those that I greet with a casual hello however I do not think of them as friends just acquaintances My computer desk keeps on shaking while I Am typing this I should have brought a sturdier desk but then how would it fit in here This place is so cramped Luckily this weekend I Am going back home to Houston to get a bunk bed and also to see my family and girlfriend of course I miss my family very much especially my dad He have always been the strict yet caring figure that shadows every minute of my life up till now Now I have to be on my own that is the only way that I will learn to grow up I know he must be worried out of his mind right now wondering what I Am doing up here wondering how my health is wondering how I Am doing in school wondering how late I sleep wondering how many times I eat in a day etc I know he worries but the best I can do is call him every night just to let him know that I Am doing fine I cannot go back home if I live with him then he will take care of everything for me then I will never learn to grow up I miss my mom too but I Am more attached to my dad because he is the only other male in our family of seven The buses hear run pretty frequently I just heard another bus pass by our apartment this is pretty convenient My roommate just got home he is the bossy but cool one out of us I Am like the normal one well besides my random outburst of course My other roommate is the cool quiet subtle one Its always fun to pick on him but we know our limits Oh yea VSA meeting is today maybe I will get to know more people Then again I doubt it because I Am always quiet and reserved so I guess I will not talk much Well I Am going to wrap this up because the timers about to run out After this I Am going to head to class then its VSA meeting '],\n",
       " 'input_ids_roberta': tensor([[   0, 1437, 1437,  ...,    1,    1,    1],\n",
       "         [   0,  100,   95,  ...,    1,    1,    1],\n",
       "         [   0,  534, 1264,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   0, 1208,  251,  ...,    1,    1,    1],\n",
       "         [   0, 2387, 4312,  ...,    1,    1,    1],\n",
       "         [   0, 8346,   38,  ...,    1,    1,    1]]),\n",
       " 'attention_mask_roberta': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids_roberta': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'input_ids_bert': tensor([[ 101, 4931, 2092,  ..., 1996, 4274,  102],\n",
       "         [ 101, 1045, 2074,  ..., 2043, 1045,  102],\n",
       "         [ 101, 2908, 2007,  ..., 2021, 1999,  102],\n",
       "         ...,\n",
       "         [ 101, 2154, 2146,  ..., 2026, 2132,  102],\n",
       "         [ 101, 2026, 4301,  ...,    0,    0,    0],\n",
       "         [ 101, 2092, 1045,  ..., 1996, 2069,  102]]),\n",
       " 'attention_mask_bert': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'token_type_ids_bert': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([[1., 0., 1., 1., 0.],\n",
       "         [0., 1., 0., 1., 0.],\n",
       "         [0., 1., 0., 1., 0.],\n",
       "         [1., 0., 1., 0., 1.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [1., 1., 0., 1., 1.],\n",
       "         [1., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 1., 0.],\n",
       "         [1., 1., 0., 1., 1.],\n",
       "         [1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [0., 1., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 1., 0.]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch=next(iter(train_data_loader))\n",
    "print('Input ids shape:',batch['input_ids_roberta'].shape, 'Attention mask shape:',batch['attention_mask_roberta'].shape, 'Token type ids shape:',batch['token_type_ids_roberta'].shape, 'Targets shape:',batch['targets'].shape)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, PRE_TRAINED_MODEL_NAME, num_classes=5, dropout=0.3):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True, output_attentions=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear = torch.nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        attentions=output.attentions\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tq\n",
    "# Training of the model for one epoch\n",
    "\n",
    "def train_model(training_loader, model, optimizer,loss_fn, scheduler=None, device=device, model_name='bert'):\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    for _, data in loop:\n",
    "        ids = data[f'input_ids_{model_name}'].to(device, dtype = torch.long)\n",
    "        mask = data[f'attention_mask_{model_name}'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data[f'token_type_ids_{model_name}'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        \n",
    "        # forward\n",
    "        outputs,_ = model(ids, mask, token_type_ids) # (batch,predict)=(32,8)\n",
    "        ids = ids.cpu().detach().numpy()\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs==targets)\n",
    "        num_samples += targets.size   # total number of elements in the 2D array\n",
    "       \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of the model\n",
    "\n",
    "def eval_model(validation_loader, model,loss_fn,model_name='bert'):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for btch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data[f'input_ids_{model_name}'].to(device, dtype = torch.long)\n",
    "            mask = data[f'attention_mask_{model_name}'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data[f'token_type_ids_{model_name}'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs,_ = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss combines a Sigmoid layer and the BCELoss in one single class. \n",
    "# This version is more numerically stable than using a plain Sigmoid followed \n",
    "# by a BCELoss as, by combining the operations into one layer, \n",
    "# we take advantage of the log-sum-exp trick for numerical stability.\n",
    "def loss_fn(preds, labels):\n",
    "    return torch.nn.BCEWithLogitsLoss()(preds, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "from collections import defaultdict\n",
    "def train(PRE_TRAINED_MODEL_NAME,model_name):\n",
    "    model=BERTClass(PRE_TRAINED_MODEL_NAME)\n",
    "    model.to(device)\n",
    "\n",
    "    # setting the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)    \n",
    "\n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "    print(f'{PRE_TRAINED_MODEL_NAME}')\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print(f'Epoch {epoch}/{EPOCHS}')\n",
    "        model, train_acc, train_loss = train_model(train_data_loader, model, optimizer,loss_fn=loss_fn,model_name=model_name)\n",
    "        val_acc, val_loss = eval_model(val_data_loader, model,loss_fn=loss_fn,model_name=model_name)\n",
    "\n",
    "        print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        # save the best model\n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(),CHECKPOINTPATH+f'_{PRE_TRAINED_MODEL_NAME}.bin')\n",
    "            best_accuracy = val_acc\n",
    "\n",
    "    print(f'{PRE_TRAINED_MODEL_NAME} Best accuracy: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9202e1dfec924f22920f582b903d4ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6934, val_loss=0.6919 train_acc=0.5068, val_acc=0.5242\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0d487a9eea400dbd7b5c20ebb2fe3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6918, val_loss=0.6902 train_acc=0.5149, val_acc=0.5371\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0e2fe4347b48c8bc1e883f16e9827a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6862, val_loss=0.6840 train_acc=0.5445, val_acc=0.5694\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca5c4b0de6a48aab9c1745ae233a519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6719, val_loss=0.6775 train_acc=0.5793, val_acc=0.5935\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a44d90c4d64405bf70f91492671e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6449, val_loss=0.6899 train_acc=0.6234, val_acc=0.5685\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d4b4b8dcda43c18687e62cbd5b67c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6018, val_loss=0.7056 train_acc=0.6745, val_acc=0.5831\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b77704557d042a099ad53a1804ff701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5462, val_loss=0.7684 train_acc=0.7283, val_acc=0.5468\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edc09c57f4341c49cd99f37e05e29e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4904, val_loss=0.7689 train_acc=0.7727, val_acc=0.5532\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51600d18389407ca86acd9bf1d6bec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4334, val_loss=0.8258 train_acc=0.8070, val_acc=0.5573\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e56196df82343f1b3cbb86591f49e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.3820, val_loss=0.9159 train_acc=0.8363, val_acc=0.5492\n",
      "roberta-base Best accuracy: 0.5935483870967742\n"
     ]
    }
   ],
   "source": [
    "# train both models\n",
    "train(PRE_TRAINED_MODEL_NAME_1,\"roberta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc         # garbage collect library\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9960ec74b0d4944a83219d02dbebfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6973, val_loss=0.6856 train_acc=0.5128, val_acc=0.5548\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2161ef7fd81e442788ec7d55cd2948cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6888, val_loss=0.6891 train_acc=0.5372, val_acc=0.5476\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe9f3c0ce224fe38feb2827e0900bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6826, val_loss=0.6775 train_acc=0.5560, val_acc=0.5645\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2388c9ed0b0f4ead9a59ff927aa7432d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6694, val_loss=0.6847 train_acc=0.5855, val_acc=0.5758\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065b7387acca44d7abf7d8d30e62052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6522, val_loss=0.6829 train_acc=0.6168, val_acc=0.5718\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd15b1f8f90415f847b2936e2c36d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6275, val_loss=0.6923 train_acc=0.6523, val_acc=0.5766\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6daf657a5b0f42529a252ee6f8fb080b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5900, val_loss=0.6738 train_acc=0.6936, val_acc=0.5879\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471510132a0741ddab79582f5f40df85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5516, val_loss=0.6903 train_acc=0.7293, val_acc=0.5903\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e164cee308b740b08f99866d70d91246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5108, val_loss=0.7157 train_acc=0.7625, val_acc=0.5847\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa71880b0874f83ac4574968d34c953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4635, val_loss=0.7611 train_acc=0.7926, val_acc=0.5677\n",
      "bert-base-uncased Best accuracy: 0.5903225806451613\n"
     ]
    }
   ],
   "source": [
    "train(PRE_TRAINED_MODEL_NAME_2,\"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions\n",
    "def get_predictions(model,data_loader,model_name):\n",
    "    model.eval()\n",
    "    text=[]\n",
    "    predictions = []\n",
    "    predictions_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            text.extend(data['text'])\n",
    "            ids = data[f'input_ids_{model_name}'].to(device, dtype = torch.long)\n",
    "            mask = data[f'attention_mask_{model_name}'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data[f'token_type_ids_{model_name}'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs,_ = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            predictions.extend(outputs.cpu().detach().round())\n",
    "            real_values.extend(targets.cpu().detach())\n",
    "            predictions_probs.extend(outputs.cpu().detach())      \n",
    "    predictions = torch.stack(predictions)\n",
    "    predictions_probs = torch.stack(predictions_probs)\n",
    "    real_values = torch.stack(real_values)\n",
    "    return text,predictions,predictions_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_metrics(model,test_data_loader,model_name):\n",
    "    # get predictions for the test data\n",
    "    text, predictions, prediction_probs, real_values = get_predictions(model, test_data_loader,model_name)\n",
    "    accuracy = accuracy_score(real_values.view(-1), predictions.view(-1))\n",
    "    accuracy_scores = {}\n",
    "    for i in range(len(target_list)):\n",
    "        accuracy_scores[target_list[i]] = accuracy_score(real_values[:,i], predictions[:,i])\n",
    "    report=classification_report(real_values, predictions, target_names=target_list)\n",
    "    print(f\"Accuracy {accuracy}\")\n",
    "    print(f'Accuracy Scores {accuracy_scores}')\n",
    "    print(f\"classification_report{report}\")\n",
    "    return accuracy,accuracy_score,report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load roberta\n",
    "model_roberta=BERTClass(PRE_TRAINED_MODEL_NAME_1)\n",
    "model_roberta.load_state_dict(torch.load(CHECKPOINTPATH+f'_{PRE_TRAINED_MODEL_NAME_1}.bin'))\n",
    "model_roberta.eval()\n",
    "model_roberta.to(device)\n",
    "\n",
    "# load bert\n",
    "model_bert=BERTClass(PRE_TRAINED_MODEL_NAME_2)\n",
    "model_bert.load_state_dict(torch.load(CHECKPOINTPATH+f'_{PRE_TRAINED_MODEL_NAME_2}.bin'))\n",
    "model_bert.eval()\n",
    "model_bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5609756097560976\n",
      "Accuracy Scores {'cEXT': 0.5447154471544715, 'cNEU': 0.556910569105691, 'cAGR': 0.5203252032520326, 'cCON': 0.5772357723577236, 'cOPN': 0.6056910569105691}\n",
      "classification_report              precision    recall  f1-score   support\n",
      "\n",
      "        cEXT       0.60      0.69      0.64       144\n",
      "        cNEU       0.55      0.64      0.59       123\n",
      "        cAGR       0.53      0.58      0.55       126\n",
      "        cCON       0.60      0.53      0.56       126\n",
      "        cOPN       0.62      0.50      0.56       121\n",
      "\n",
      "   micro avg       0.58      0.59      0.58       640\n",
      "   macro avg       0.58      0.59      0.58       640\n",
      "weighted avg       0.58      0.59      0.58       640\n",
      " samples avg       0.58      0.60      0.54       640\n",
      "\n",
      "Accuracy 0.5552845528455285\n",
      "Accuracy Scores {'cEXT': 0.5487804878048781, 'cNEU': 0.5609756097560976, 'cAGR': 0.5284552845528455, 'cCON': 0.556910569105691, 'cOPN': 0.5813008130081301}\n",
      "classification_report              precision    recall  f1-score   support\n",
      "\n",
      "        cEXT       0.60      0.69      0.64       144\n",
      "        cNEU       0.56      0.54      0.55       123\n",
      "        cAGR       0.54      0.58      0.56       126\n",
      "        cCON       0.57      0.57      0.57       126\n",
      "        cOPN       0.55      0.78      0.65       121\n",
      "\n",
      "   micro avg       0.56      0.63      0.60       640\n",
      "   macro avg       0.56      0.63      0.59       640\n",
      "weighted avg       0.56      0.63      0.60       640\n",
      " samples avg       0.58      0.64      0.56       640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5552845528455285,\n",
       " <function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>,\n",
       " '              precision    recall  f1-score   support\\n\\n        cEXT       0.60      0.69      0.64       144\\n        cNEU       0.56      0.54      0.55       123\\n        cAGR       0.54      0.58      0.56       126\\n        cCON       0.57      0.57      0.57       126\\n        cOPN       0.55      0.78      0.65       121\\n\\n   micro avg       0.56      0.63      0.60       640\\n   macro avg       0.56      0.63      0.59       640\\nweighted avg       0.56      0.63      0.60       640\\n samples avg       0.58      0.64      0.56       640\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get metrics for both models\n",
    "get_single_metrics(model_roberta,test_data_loader,\"roberta\")\n",
    "get_single_metrics(model_bert,test_data_loader,\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4565, 0.5897, 0.2768, 0.3034, 0.9305],\n",
      "        [0.7815, 0.2013, 0.8386, 0.7884, 0.1424],\n",
      "        [0.6110, 0.3704, 0.6355, 0.5999, 0.7785],\n",
      "        ...,\n",
      "        [0.7636, 0.6614, 0.4687, 0.5767, 0.7396],\n",
      "        [0.3764, 0.4350, 0.6789, 0.7481, 0.7647],\n",
      "        [0.6636, 0.3018, 0.8050, 0.7310, 0.0875]])\n",
      "tensor([[0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "_, _, prediction_probs, real_values = get_predictions(model_roberta, test_data_loader,\"roberta\")\n",
    "_, _, prediction_probs, real_values = get_predictions(model_bert, test_data_loader,\"bert\")\n",
    "\n",
    "weight1 = 1  # Weight for model 1\n",
    "weight2 = 1  # Weight for model 2\n",
    "\n",
    "# Combine probabilities using weighted averaging\n",
    "combined_probs = (weight1 * prediction_probs + weight2 * prediction_probs) / (weight1 + weight2)\n",
    "print(combined_probs)\n",
    "predictions= combined_probs.round()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5552845528455285\n",
      "Accuracy Scores {'cEXT': 0.5487804878048781, 'cNEU': 0.5609756097560976, 'cAGR': 0.5284552845528455, 'cCON': 0.556910569105691, 'cOPN': 0.5813008130081301}\n"
     ]
    }
   ],
   "source": [
    "# ensemble metrics\n",
    "accuracy = accuracy_score(real_values.view(-1), predictions.view(-1))\n",
    "accuracy_scores = {}\n",
    "for i in range(len(target_list)):\n",
    "    accuracy_scores[target_list[i]] = accuracy_score(real_values[:,i], predictions[:,i])\n",
    "# classification_report=classification_report(real_values, predictions, target_names=target_list)\n",
    "print(f\"Accuracy {accuracy}\")\n",
    "print(f'Accuracy Scores {accuracy_scores}')\n",
    "# print(f\"classification_report{classification_report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
