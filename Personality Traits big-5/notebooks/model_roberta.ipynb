{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 00:59:22 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 28%   45C    P8     7W / 180W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# check if local env or paperspace\n",
    "import os\n",
    "if os.path.exists('/datasets/essays/essays_cleaned.csv'):\n",
    "    dataset_path='/datasets/essays/essays_cleaned.csv'\n",
    "else:\n",
    "    dataset_path='../data/essays_cleaned.csv'\n",
    "df = pd.read_csv(dataset_path,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
    "# Hyperparameters\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid\n",
    "# intialize tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, df, max_len,target_list):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = list(df['TEXT'])\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.targets = self.df[target_list].values\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        text_length = len(text)\n",
    "        middle_start = text_length // 3\n",
    "        middle_end = 2 * (text_length // 3)\n",
    "        text_middle = text[middle_start:middle_end]\n",
    "        text=text_middle\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target list is the list of big 5 personality traits that we want to predict\n",
    "target_list= list(df.columns[1:])\n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_dataset = EssayDataset(df_train, MAX_LEN, target_list)\n",
    "valid_dataset = EssayDataset(df_valid, MAX_LEN, target_list)\n",
    "test_dataset = EssayDataset(df_test, MAX_LEN, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['pretty thick beer goggles on so I do not really know if she was as good looking as I thought OH well That just shows my dedication to crew IF I give up getting some then being on the crew team must be very important to me I felt like I was in too This girl was just absolutely digging me that is a shame and then on saturday I went to a lakehouse that was a neither a on a lake b not even a house it was a freakin streamcreek apartment that sucked These girls that were there were pretty annoying ONe was really cute but she was kind of snobbish I do not know Id like to talk to her but these girls were all too snobbish for me I was hoping maybe there would be some down to earth girls but apparently not I hope I meet a good girl pretty soon I want a girlfriend but I think I Am trying too ha',\n",
       "  'd have taken advantage of that earlier and done this assignment then My attention span is short I keep finding myself drawn into her conversation It makes me think about talking to my friends on the phone which eventually leads to me reminiscing about some past event involving my friends The funny thing about my situation now is that only actual people talking interrupts my thought process The television does not disturb me at all I have gotten very good at tuning it out Maybe its because I have made it a habit of doing just about everything with the television on including sleeping I am growing impatient with the ',\n",
       "  'n that you had to much time you did not know what to do with it That has not been my problem lately the only problems I have had all are about having not enough time Every time I talk to my girlfriend back home a senior in high school I tell her to take advantage of having a lot of time on her hands because college is no where near as easy I know older people had told me the same but I really never believed them and always felt as though they were exagerating Oh yeah I am getting off topic but this library is just boring me I feel that everyone is ahead of me on everything I feel like they all grasp the ideas that I do not In high school I was ahead of everyone of my friends but now it seems to me like all of my friends are ahead of me Maybe that is just me overthinking I can now smell some food as though somebody has brought something into the library It smells like a piece of chocolate Maybe it is a candybar of some sort I do not really care because that smell kind of nauseates me I am not a real big fan chocolate but I do love chocolate ice cream Something ',\n",
       "  'pretty complicated if you really think about it long enough Sometimes when you meet someone of the opposite you are attracted to them right away But what really attracts you to that person  Is it the way they smile The way they simply look at you  what is it really  After its obvious that the attraction is there you have to move on to the next step Which for some people is the hardest part Its the whole approaching the person thing You want to be yourself but then at the same time you want to do something to impress the person so you might',\n",
       "  'it at community college maybe yeah yeah yeah ogh my roommate kareem hes a friend of mine from Houston its like we have been together 247 since we moved in together and its really kind of annoying i feel bad because he was like one of my best friends and now its like I do not want to be around him anymore so Ill tell you what I want what I really really want if you want my future forget my past if you want to get with me better make it fast SPICE girls SUCK I Am listening to a new CD right now that I got at the radio station at UT91 7 KVRX I Am applying for a position there as a DJ or something that should be thrilling I just want to be heard GUYS  if you wannabe my lover you got to get with my friends if the song was called got to get it would be COOLER  this is  going to be like so long I still have 11 minutes what am I going to do  there was this girl in front of us in PSYCH today who my friend says is really stuck up and superficial so I kept whispering superficial to see if she would turn around she never did so does this mean she is NOT superficial  or she IS she just does not know it as often is the case I think this paper rocks man I bet you all get some really freaky ones like the end of the world  is coming to Austin  the MTV video music awards is on in 2 hours1 what if I get like caught in the net and I cannot watch the awards that would suck I Am going home in 2 weeks my friend is having a bisexual encounter tonite that is pretty weird huh  I do not really understand the concept of bisexuality is not it either on or the other well I think of myself as bisexual and HEY NOW you cannot go printing this all around the class becau',\n",
       "  'm Ben my exboyfriend Not going to waste my time on him anymore why do people cheat on each other I woant  ever understand that And what makes him think that just by sayng I Love You htat I am just going to forget everything Forget hijm  at  least I have met someone new that I can tlak to heis so sweet and all but I think I should call it off nothing ever works for me  and I ned to concentrate more on my shcool work I do not know what to do about guys I need to meet some more girls to hang out  with but the only ones I see are all snoby sorority girls  a nd that just is not me at all Kyle is such a sweet guy and all but should  I tell him htat I need to slow down and work on my shcool or should I just let things go and see what happens I Am scared to just go with  the flow I really like this guy a lot and the last time I felt this way I got hurt and  that pain is still here Everytimt Ben calls I  get a ',\n",
       "  ' go out with And even better he knows everybody either doormen or bartenders or owners or all of the above So essentially he can get us in with no problem not like there is much of a problem anyway being girls because all the fun ones are 21 and over Shhh do not tell my mom I told her i stopped going to them She found out once because of jessicas crazy parents oooh i cannot stand that But we still continued and had a jolly time Everyone there knows us and that were underage Its great really the chicago cops the whole town is nuts Everyone thinks texas is laid back which it is but really the chicago cops really just do not give a shit There are so many undercover cops at the doors who know us and even ones who work outside just patrolling the streets But they look for more trouble than just underage drinkers Kind of funny One night about 8 cops were leaving this really big club downtown CHicago and stopped us on our way out and started asking questions Like hold we were and blah blah but they were all drunk and laughing and traveling around in a paddy wagon We said we were 19 were 18 i do not know why we said that And they asked for our numbers and offered to take us out with them But that is just how all of them are Its sooo weird But unless you know people its not so easy being underage Chicagos not a big underage drinkers fest Which i really do think the law is ridiculous If the drinking age were persay 18 then i do not think there would be such a huge   binge drinking   problem for kids when they leave to college Because if kids are allowed to drink with parents supervision which in IL its illegal then the parents can monitor it and teach their kids That way leaving for college is not such a huge shock and kids will not get buck wild because they can drink without parents grounding them Most of my friends parents we',\n",
       "  'g 5 of the clock in the morning I guess one of the reason I wanted to join is because to challenge myself and make some new friends More than 100 people from my graduating class comes to UT now Before school ended I got very tired of many of the people I felt they were very fake I came to UT hoping to make some friends I still wanted to keep my old friends though I truly love them They mean so much to me What I Am trying to say is that its hard to find friendships like that Everyone on my floor',\n",
       "  ' its not really doing me any good I just want to have so much fun at college and I totally am I really like all the friends I have made so far I have a really fun group of girls in my dorm we call ourselves kins klan lol we all live at kinsolving I m so happy that we are friends and that we have so much fun together I cannot wait to start making some good guy friends it sucks because they are always so drunk whenever I meet them so they never remember my name even though we have met like 20 times but that happens to everyone I Am not really hungry anymore but I just want to go to lunch and see my friends o crap I have to go to stupid calculus after that and I really do not want to go and watch my ta get chalk dust all over himself he is such a mess and I cannot understand him at all I hope I am able to figure out the last five problems today so that I do not have to do it anymore and I can just submit the answers and get one thing done o yah I have to ask eklly about chemistry I hope she remembers how to convert celcius to ferenhright and how to calculate water displacement because I definitely do not remember ooo my fingers are tired but I like typing lol I Am really enjoying reading all this psychology stuff its fun but at the same time I Am totally self diagnosing myself I Am like oo my neurotransmitters are so messed up and I need drugs for these and I need drugs for these I think its kind of funny at the same time I cannot wait until our mixer tomorrow night with sae it is going to be awesome and then the match on friedya I really want to dress up whitetrash but that will be weird wehn we go to ato which we willdef do because I Am a totally ato groupie and I Am still dressed in my white trash outfit the music is out how annoying it must not have good reception I never listen to music here anymore and I barely watch tv I kind of like that there is just too much other stuff to do and if th',\n",
       "  'ed out of her house well she left actually but she had gotten in a really bad fight with her mother and her mom had called her a slut I do not think I could handle my mother calling me a name like that I think I have a very good relationship with my mother but I know that I do not tell her everything that goes on in my life I Have noticed that I have most of my secrets spread out through a bunch of people like no one person knows all of my secrets I guess that is because I Am afraid to trust one person so much but I do not think that is a bad thing I just think that I Am afraid that if one person knows me totally they will turn on me and tell everyone my personal business I guess that is because in junior high I had a group of friends that turned on me and I do not think I ever forgave them I went my own way in high school but to this day I still think of their betrayel and I can still feel the pain from it Even though I have not always had the best friends in the world if I have a falling out with a friend I usually will make up with them later on in life We might not be friends again but the air will be cleared and we can be civil to each other That also works for my exboyfriends it seems like after about a year we will begin to talk again and everything is ok I Have always been able to make friends with guys better than I have with girls I really do not think I Am that much of a tomboy but I just think they are easier to get along with and because less drama My first year in college was the first time I think I made more girl friends but that also might be because I Am in nursing and there is a serious shortage of men in that field I do not mind girls I just think they have more issues than guys I also think its rea',\n",
       "  ' hot attractive she is But now I Am thinking about another girl that I have a crush on Today she invited me over to her apartment to have some burgers with her and her roommates I Am not sure if she knows how I feel but I Am having a hard time reading her Usually I Am pretty good about being able to tell if a girl is feeling me but she seems to be different which does not make things any easier I also got to talk to my roommates about things that had been on my mind which was refreshing because I had been wanting to talk to them for a while Things seem to be really good between us which is not too surprising because were all pretty decent guys though one does tend to get to us sometimes I am going blank right now but hopefully something will come to mind On yeah I got to talk to my parents today which was not too bad My dad did not say much because he had football practice but he said that the team was coming around which is always good to hear because he puts a lot of effort and ',\n",
       "  'om stockholders were rich but now amazon died down a bit Ok I did shop at amazon com once I Am not a big fan of sending out my credit card number across cyber space I just do not feel safe its that insecurity paranoia I have But I guess shopping online is sort of fun But I feel that things are getting much more easier and I Am getting much lazier I come to UT and everything is like technologized if that is such a word but I mean I can pay may bills online and register my classes online and heck you got a website to make my life easier I Am for extra help but to mean all the stuff to make my life easier is bumming me out I wonder if I am as smart without the technology because its like the other people living way before or going to school at UT before all the new tech stuff was offered to them I mean every student almost has to have an email account I still do not know how to get to mine at all I Am so computer illiterate and I want to be a MIS major is not that an oxymoron One day I know that I will have to eventually have to buckle down and start learning how to work a computer and not ask people to help me with it I told my teacher once that if my computer ever breaks ',\n",
       "  ' was not here and my roommate did not know my cell number or my friends which is where I was at So I had to lay down the law and tell her not to take out her anger on my roommate I was mad also because I gave her my cell number the day before but she did not write it down So we argued for a while about that then she told me something that her and her friend did that I did not approve of So I was instantly angered because I have a short temper to begin with It was a big deal really but I do not let anything go by even the smallest fuckups So we started to have any even greater argument than the first one and did not end on a good note She decided that she did not want to talk to me if we were going to argue I acted like I did not care so we just ended it the',\n",
       "  'y what i want to do i love watching ut play no matter what sport i was athletic in high school i wish i would have listened to my dad when he said   you should practice you could get a scholarship   i could have the demand for girl golfers is pretty high i did not listen oh well cannot cry over spilt milk i miss my parents i know they miss me my mom calls alot but i am glad she calls me because i would fell kind of foolish is i always called home you know everytime i visit i get teary eyed i know that sounds stupid but its getting better though i love this school though the only school i ever wanted to go to been a fan forever everyone else in my family is an aggie i will be the first longhorn in family history pretty cool i think i wish i could do more with my room its too small and my closet man i have no words for that but i like it here its close to school and everything else for that matter the walking across campus kind of sucks but that is ok because its ut campus tha',\n",
       "  'nting anything I hope the camera I bought on Saturday is going to be here this week I bought the new Sony DSCT1 which is the credit card size digital camera sony makes I cannot wait to get it I have been wanting the camera now for six months I miss my 1967 Camaro I restored it this year and during the summer I went to Florida and traded it in for a 2002 SS Camaro Wow what a change that car hauls butt and is nice because it has air conditioning which the 67 did not I still have my 68 Camaro and I will probably be wanting to restore that one pretty soon My dream is to be a Orthodontist and I cannot stop thinking about if and when I will realize if this dream could be a reality I know I have to wait 3 more years to apply to dental school but I cannot tell if I will have high enough grades to get accepted I really hope my Chemistry class is not as hard as I am making it out to be I took Chemistry AP last year in high school so I would be prepared but I fee',\n",
       "  ' I could have broken that lie detector test easy and in the mean time made people laugh too Hmmm I am getting hungry right now and now my roomate has just came in and keeps asking me what I am doing and now I cannot concentrate anymore I really enjoy watching Michael Jordan play basketball and I want to get that video that is out on his greatest moments Dang my room is messsy I really need to clean it up oh well I wonder if the MTV music awards are going to be any good tonight sounds like it would be interesting for a big get together with friends and s'],\n",
       " 'input_ids': tensor([[    0, 28674,  7992,  ...,     1,     1,     1],\n",
       "         [    0,   417,    33,  ...,     1,     1,     1],\n",
       "         [    0,   282,    14,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   219,    99,  ...,     1,     1,     1],\n",
       "         [    0,  3999,   154,  ...,     1,     1,     1],\n",
       "         [    0,    38,   115,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([[1., 1., 1., 0., 0.],\n",
       "         [1., 0., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 0., 1., 1., 1.],\n",
       "         [0., 1., 1., 0., 0.],\n",
       "         [1., 1., 0., 0., 0.],\n",
       "         [0., 1., 1., 0., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 0.],\n",
       "         [1., 0., 1., 1., 1.],\n",
       "         [1., 1., 0., 1., 0.],\n",
       "         [0., 1., 1., 0., 1.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [1., 0., 1., 1., 0.],\n",
       "         [0., 1., 0., 1., 0.],\n",
       "         [0., 0., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = transformers.AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss combines a Sigmoid layer and the BCELoss in one single class. \n",
    "# This version is more numerically stable than using a plain Sigmoid followed \n",
    "# by a BCELoss as, by combining the operations into one layer, \n",
    "# we take advantage of the log-sum-exp trick for numerical stability.\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model for one epoch\n",
    "def train_model(training_loader, model, optimizer):\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    for batch_ix, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids) # (batch,predict)=(32,8)\n",
    "        ids = ids.cpu().detach().numpy()\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs==targets)\n",
    "        num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        #loop.set_description(f\"\")\n",
    "        #loop.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(validation_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for btch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedf7a9c80d04f5791befdb2c91b2b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6937, val_loss=0.6900 train_acc=0.5091, val_acc=0.5267\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87416278ef7b450ba192b102c7a88c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6876, val_loss=0.6928 train_acc=0.5442, val_acc=0.5477\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7e95dc57224b7f853ceb50d05ab640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6738, val_loss=0.6822 train_acc=0.5804, val_acc=0.5639\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9507cf0832417d92fae3b4114e589f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6508, val_loss=0.7053 train_acc=0.6176, val_acc=0.5714\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bd7538d38f4024bc7971ed1a1ce16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6182, val_loss=0.7090 train_acc=0.6535, val_acc=0.5757\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91a34058b6841b5879cc3c7cf1ccfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5797, val_loss=0.7205 train_acc=0.6999, val_acc=0.5784\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319b629de4ee4e3c981ab7c53915ba4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5352, val_loss=0.7231 train_acc=0.7345, val_acc=0.5790\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712395419eea45c5b163d9f14b16df2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4817, val_loss=0.8009 train_acc=0.7782, val_acc=0.5741\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48fd5e60d3c4e83bb25663dd4875847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4321, val_loss=0.8222 train_acc=0.8078, val_acc=0.5698\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04b45b204b2464997731c848237e466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.3885, val_loss=0.8791 train_acc=0.8353, val_acc=0.5590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    # save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(),\"ckpts/MLTC_roberta_model_state.bin\")\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "model = BERTClass()\n",
    "model.load_state_dict(torch.load(\"../checkpoints/MLTC_roberta_model_state.bin\",map_location=torch.device('cpu')),strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5843243243243244, 0.7146516864498457)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_acc, test_loss = eval_model(test_data_loader, model, optimizer)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model,data_loader):\n",
    "    model.eval()\n",
    "    text=[]\n",
    "    predictions = []\n",
    "    predictions_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            text.extend(data['text'])\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            predictions.extend(outputs.cpu().detach().round())\n",
    "            real_values.extend(targets.cpu().detach())\n",
    "            predictions_probs.extend(outputs.cpu().detach())      \n",
    "    predictions = torch.stack(predictions)\n",
    "    predictions_probs = torch.stack(predictions_probs)\n",
    "    real_values = torch.stack(real_values)\n",
    "    return text,predictions,predictions_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test data\n",
    "text, predictions, prediction_probs, real_values = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: torch.Size([370, 5]), real values shape: torch.Size([370, 5]), text length: 370, prediction_probs shape: torch.Size([370, 5])\n"
     ]
    }
   ],
   "source": [
    "# print size and shapes\n",
    "print(f\"predictions shape: {predictions.shape}, real values shape: {real_values.shape}, text length: {len(text)}, prediction_probs shape: {prediction_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cEXT       0.53      0.79      0.64       178\n",
      "        cNEU       0.60      0.61      0.60       190\n",
      "        cAGR       0.55      0.79      0.65       195\n",
      "        cCON       0.57      0.85      0.68       186\n",
      "        cOPN       0.62      0.65      0.63       190\n",
      "\n",
      "   micro avg       0.57      0.74      0.64       939\n",
      "   macro avg       0.57      0.74      0.64       939\n",
      "weighted avg       0.57      0.74      0.64       939\n",
      " samples avg       0.58      0.71      0.60       939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(real_values, predictions, target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      3\u001b[0m accuracy_scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtarget_list\u001b[49m)):\n\u001b[1;32m      5\u001b[0m     accuracy_scores[target_list[i]] \u001b[38;5;241m=\u001b[39m accuracy_score(real_values[:,i], predictions[:,i])\n\u001b[1;32m      6\u001b[0m accuracy_scores\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_list' is not defined"
     ]
    }
   ],
   "source": [
    "#  calulate accuracy using sklearn for each trait\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_scores = {}\n",
    "for i in range(len(target_list)):\n",
    "    accuracy_scores[target_list[i]] = accuracy_score(real_values[:,i], predictions[:,i])\n",
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
