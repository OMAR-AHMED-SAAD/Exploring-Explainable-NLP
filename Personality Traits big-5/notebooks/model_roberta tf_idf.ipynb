{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 14:04:53 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   33C    P8     7W / 180W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 14:05:01 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   33C    P8     6W / 180W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# check if local env or paperspace\n",
    "import os\n",
    "if os.path.exists('/datasets/essays/essays_cleaned.csv'):\n",
    "    dataset_path='/datasets/essays/essays_cleaned.csv'\n",
    "else:\n",
    "    dataset_path='../data/essays_cleaned.csv'\n",
    "df = pd.read_csv(dataset_path,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(df['TEXT'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f17aeb6bd2749a88f0e073defac7afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71d6db58a66489e8bd13fa7b881544a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0eaa3a5b4b644c3ad1d4f75c2894912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bdf22b98c24916b068935b168dc599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b1a78faa704697a8785104b0259474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
    "# Hyperparameters\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-05\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid\n",
    "# intialize tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train, df_test = train_test_split(df, random_state=77, test_size=0.30, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_test, random_state=88, test_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, df, max_len,target_list):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = list(df['TEXT'])\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.targets = self.df[target_list].values\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        text_length = len(text)\n",
    "        middle_start = text_length // 3\n",
    "        middle_end = 2 * (text_length // 3)\n",
    "        text_middle = text[middle_start:middle_end]\n",
    "        text=text_middle\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs['token_type_ids'].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[idx]),\n",
    "            'tf_idf_features': torch.FloatTensor(vectorizer.transform([text]).toarray().flatten())\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target list is the list of big 5 personality traits that we want to predict\n",
    "target_list= list(df.columns[1:])\n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_dataset = EssayDataset(df_train, MAX_LEN, target_list)\n",
    "valid_dataset = EssayDataset(df_valid, MAX_LEN, target_list)\n",
    "test_dataset = EssayDataset(df_test, MAX_LEN, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [' first of all I Am not 18 yet so they will not let me get the fancy shmancy email account yet the other day they let me get a 2000 dollar loan which I have to pay off in 3 months or they sue me   wacky oif you ask me but then again who is  this campus is so huge no one knows anyone around here none of the professors none of the students  you smile at someone and immediately your somesort of wack job walking down the street stoopid people keep bothering me do not they know I Am trying to write   bno respect I tell you none from no body not my parents not my stoopid facking roommate who thinks hes my father  him and his girlfriend I tell yoiu first of all they are not very pretty people to begin with and then they are always here having sex and dammit it get annoying  always trying to include us not leaving alone when I do not want to ber apart of the picture always is it okay if I do not this or were goint to have sex now do not oing tin to the bed room I do not car ego and getr phreaky do not tell me about it icant ccare less  I just bugs the hell out of me when they decide well jason is feeling left out of our circle of love let us include him  and I do not want to have any thing to do with them and when they are togheter its fucking discussing they act like fuckingh middle schoolers always hopping and bopping about acting all like I love you I love yout oo let us have babies holding hands and licking lips  never a moments res',\n",
       "  'itely am not going to do that research paper why does this page have no ending widththese sentences are getting really long when am I going to finish my other hw I missed the first two days of one of my classes and now I do not even know what the hw assign are the gate outside my window keeps opening and makes this squeaking noise my roommate is worthlesshes supposed to get our dishwashing rack and get the exterminator to come but hes too lazy and all that he does is sit around and play his computer game I just looked at the clock and its only been 10 minutes20 minutes is a long time when you are thinking about itI remember when I played soccer 40 minutes seemed like 4 hours especially when you were losing I wonder what my family is doing right nowI miss my brotherhes going to come to school here next yearI cannot decide if we should live together or whatit would be a lot of fun I do not know if I could handle another year in the dormsthey suck especially the bathrooms and the foodactually everything about them is bad I wonder if anyone is going to be able',\n",
       "  'o everyday life I liked how it happened to be about what I was studying Anyway after that I shut my eyes and tried to sleep I do not think I was actually asleep but I feel less tired now I guess that was the point of my socalled nap My brother calls them power naps but I could never understand that until now When I was younger I could fall asleep at any time and still go to bed on time That has changed If I sleep during the day I cannot fall asleep as easily at night I think my nap today was not long enough for me to be awake tonight to study but Ill find that out later I am going to read some more chemistry and biology I cannot wait Right now we are learning the basics and its so boring Its the stuff the teachers in high school just skimmed over because it was not as important as other things we needed to learn Even though my classes used to be an hour and a half in high school my classes now seem longer Its odd that I keep looking at my watch Classes are only fifty minutes and it feels like forever ',\n",
       "  'of the terrorist attack on the   world trade center What i would do to stop terrorism and Bin Laden  I cannot wait for the fall semester to be over with so i could go to   Californnia and visits my cousins and relatives I missed them so   much I Am so exhausted from weight training class I Am so tired  What worse is I have to walk all the way from Gregory Gym to the   Union going up so many steps I walked so much Why does all my   classes have to be so far apart Ten minutes left hahaha When   does the fall semester offically end Hmmm  Another pretty girl   just walks in Of all the student at UT how come i only a couple of   cute girl Why is that I Am i in the wrong building or at the wrong   place or someth',\n",
       "  'ding things but everyone seems to really care I wonder how long that will last that is another thing about living here in dallas no one really cares I mean duncanville sure people do its a smaller town but if you were to go downtown and ask someone for directions they would probably think you were crazy here people are much more friendly just yesterday I was riding the bus home from walmart and you know riding the metros around here you run into all kinds of strange and interesting people and this older black guy and a group of punk kids gets on the bus and the kids were so rude to him and talking to him like he was stupid and making fun of him right to his face and everything they did not know him anymore than I did and sure it was apparent that all of his crayons were not in the box but it amazed me how disrespectful they were to him so I said something and it was not even mean I just started asking them how old they were and how much they thought they knew in the few years they had lived none of them answered of course and it really was not my place to say but they shut up now they got off the bus quickly but the older guy stayed on and talked to me for awhile he was nice enough he had some sort of speech issue it was like talking to my grandpa after his stroke but much faster I could not tell if he did not know the words or if his brain could not unscramble the words that is what my grandpa used to say he did not talk much after his last stroke and the doctors could not help him if he could not tell them what he wanted and finally one day my mom asked him why he looked angry all the time and he told her he knew what he wanted to say but when he opened his mouth to say it it came out all scrambled up so it had to sit and think about it a second time to be sure it came out right and it wore him down and it made him tired so he just did not say much he understood everything we were saying but the delay for him aggravated him so he did not want to deal with it its sad ya know he is very smart and he is seen so muc',\n",
       "  'ar to the man who calls me because when he teaches math its in Chinglesh and its hard to understand him I wonder if my math teacher is the man who keeps calling me on my cell phone  I was sleeping a minute ago and then my roommate came in and that is when I smelt the tuna again  I got my ticket for the Arkansas game today and it only took 5 minutes that was great I saw Cedric Benson and Huston Street today they were both walking with hot blondes Neil Young has creepy sideburns but he is a great musician my roommate downloaded one of those virtual girls off the internet that does a strip tease on your desktop and it kept him entertained for hours it was cool I wish I could play the violon or some instrument their w',\n",
       "  'sses because last semester I screwed up grade wise after keeping an almost 4 0 GPA for 2 and 12 semesters I have never taken psychology before in high school and I wish I had since it would make the class easier now when I take it I really cannot type continuously for twenty minutes without having a definite subject so I keep having to stop and think of something to type One of my goals this semester is to stop watching TV so often because that is what I feel screwed me up last semester since I was always starin',\n",
       "  ' one She is not like the others She does not really like Asians either She is half Japanese and half Vietnamese I am half Chinese and half Vietnamese She also happens to be in the same pyschology class She sits next to me most of the times I always sit next to my best friend Zach He is cool He knows when to party I have a lot of friends in there and most of them are from Taylor High School There so many Taylor students in there and at UT That reminds me the girl who sat next to me today would not quit laughing At first I thought she was cool Things changed as the lecture progressed The girl who sat in the row behind me 5 seats to the right wearing the red Hilfiger shirt was good looking I wish I was white so I could approach white girls with better ease Man oh man There so many pretty girls in that class Microeconomics has a lot too Especially the one two rows down from me I no',\n",
       "  ' individual Sure I can  pierce my own bellybutton but when I see another person in pain I feel twice as bad as they most likely do Oh well Ill just give Tracy the money to go  have it professionally done This pushing enter is really beginning to  frustrate me I keep typing way over the right limit and having to erase and go back and retype what I already typed I think I need a computer I  hate having to work and concentrate amongst all the hundreds of other people  here at the SMF the Smurf I smile everytime I hear that I wanted to buy a computer and I do have the money since I sold my motorcycle not by my own  choice but by my mothers I realize that was a runon sentence but I Am not  about to go back and fix it I wonder if anyone is actually going to read this or not I Am just glad that I have not had any demented thoughts since I have  started typing Not that I have those often but we all have certain weird  thoughts that we really do not care to admit having At least I hope so I  bet whoever reads this is going to think that I Am a bad person I Am really not I like to think of myself as daring and thrill seeking My mother likes to  think of me as unconventional and improper and borderline igno',\n",
       "  'eeping a responsible daily routine are at the top of my priority list right now I do not want anything to interfere with me doing an exceptional job in all of my courses It really is a big change for me being here at a university Back home school was a breeze Now I can actually feel that I Am going to have to study which will be a first for me The good side to that is that I like challenges One of my friends from back home who is really close to me is planning to move to Florida There is not anything wrong with that because hes in love I hope that hes making the right decision with the move and with her I really care about him and I would not want him to get hurt',\n",
       "  'es the scary part in the music I am listening to Firebird Suite It really is a neat song but it lacks something not sure what but something What if you started this assignment at 450 on Friday That would really be horrible but it would not be quite as bad as starting it at 441 That would actually be ironically funny Good times Something is odd Not sure what but something seems a bit strange I want some natural light I really cannot stand these fluorescent lights These buildings also are disturbing I did not live in the country but I really miss the trees I had around my house Living amongst these lonely trees is strange Its like they are put on display as something that used to be I want to go to a school that is in the trees I am talking a tree house In the trees of the Amazon That would be a sweet experience The window right Haha of course I open my window and see construction Great just great Its rather depressing Alright more than 50 complete This could really be a dangerous thing if you really tapped into your thoughts Might find something you do not want to find Hmm I hat that feeling of being hungry after you have just ate Is ate the right word there I think it might be eaten who know',\n",
       "  'ot want to Its all invouluntary This would be allot easier if I was writing in pencil I think faster when I am writing things down It helps me organize my thoughts When I speak to people I like to confuse them Sometimes I confuse them so much they believe that a conclusion was reached at the end of our conversation but when they think about it they find out that I only caused them to think of more questions I do not know why I enjoy that When we think what do we think about the most  What is easier or what needs a solution Writing about what I think is hard I am not sure if I think of too many things at once or just do not think at all I do just stop thinking and drift off into my own world and just stay there thoughtless  Feelings  mixed feelings Not sure what to do with my life Would prefer not to be in college but know that sucess comes with knowledge Psychology has interested me since I was a child I like knowing what people are thinking about That is what I do I observe But observation comes with a price Since I play the role of observer it is hard for me to actually detail what is going on in my day and not others People think that I really do not want to talk much about myself which is true to some extent What really happens is that I do not know how to organize my daily activities to be shared with others Silence is my audience when I am by myself B',\n",
       "  'sically How do girls do it  How can anyone possible walk for seven hours straight and not get tired In that respect I completely admire them wow not left with anything left to say I must have  who the hell is making so much noise in the hall way  this dorm is always too loud I just want to move to an apartment and get some privacy I hate these public  bathrooms they are so unsanitary I Am having trouble breathing now My  asthma is acting up again probably because its so cold in here I wonder what my girlfriend is doing right now She left in a hurry after she dropped me off That Is the gratitude I get for going shopping with her for so long I bet  whoever is reading this thing is hating the fact that I can type 65 words per minute and I bet she is getting tired of reading all this crap Id go crazy if I have to  read ten of these things much less over three hundred Man I cannot breath I need to get my inhaler I think I need to go to the bathroom too but I cannot leave this computer to do it Oh boy what is life getting to  wow only ten more minutes before I am done  and I still do not know what to write about Those  people sure are making a lot of noise out there in the parking lot OH I wonder who won the UT football game  I guess we did since the tower is lit  up orange but I wonder what the score was I think I need to ask someone Not that I actually care about football in t',\n",
       "  ' is terrible   then i scurried off to class Little did i know that what i had dismissed as simply governmentpolitical jumble would have such a severe impact on America and myself When i walked into my class i was met with the same constant murmur it seemed to be everywhere I sat down in my seatfront row and looked up to see that CNN was being shown on a huge screen at the front of the classroom  My mouth dropped in horror and terror filled me as i watched with my very own eyes one of the planes slam into a huge tower It was then that i realized how serious this thing was Then the second plane and after that the third America was being attacked That phrase played inside my head fueling the growing terror inside me America was attacked and would never be the same again It was then that i realized something else My mom was out of town She had left on a business trip for the week to was it Virginia West Virigina It was an annual trip and i remember her saying it wa',\n",
       "  ' so you will just have to live with all my shitty typos so there ha  yoo cannot stop me because you just said write what on your mind and what if there are a lot of rtypes in my mind  hihn  huh  what you going to do about that  this charis too low there we go that is better but now it will not rock htere fixed that too ok now this is linda wierd because my legds are pusing against the bottom of the desk which sucks because I cannot lean bakc as much and my wrists are stargting to get tired because they have nothing to lean on I have no where for mu wrists theya re ruined damn my desk for now havign a wrist rest ok this chari is too high now and mt arpmits are sweating and I have no idea why thta happens a lot and I hate it it makes me look like a nervous fool evcen if I am ok chair lowered back to where it origicnlly was ok now I Am not even looking at the words I type I Am just staring at my fingers as they type the words and not relallyt thinking about what I Am typeing that sit be interesting to see wat kind of crasyt shit comes of out omy  head I know what kind of mistakes I Am makeing but I Am trying tnot tot chrorect thim and now I Am tryinf nor ro ecen hink just let my ifngers flow and let them do the talkjing not me oand now my eytes are closed that is not workingbla bla bla bla bla I Am still writing now how do you like that  you are not even going to read therse are you  I just know it your just doing this for some reason that no one knows yeet but it will be so obvious after you explain it that everyone or at least me will fdeel stupid you know kind of like that lie detector tesxzt if you had explained it before then it would not have worked if I knew how a lie detector test worked then it would not really be that hard to beat and since you said that they are so accurate that is making us think that they work all the time so then if we ever get convicted of a crime or at least on trieal then well think that the lie detector works and we will not be able to fool it but then you told ous how it worked and so now we know that is it is not har as accurate as you originally tol us that it would be ok m',\n",
       "  'and this year I forgot every thing and I was having to borrow landry detergent from a freshmen to do my landury wow huh that is wierd you thaink that I would be more prepared but I not worried about it iit is fine that way I get to meet more people and I will bea able to buy some laundry detergent later and one daay when they are out they can borrow some form me Well my room is not totally done and I want to clean it but I have to much other stuff to do I wish that Anna was really happy right now she is going trough a hard time and I want her to forget all this silly stuff and just trust that she will be alright I know that God will take care of her My boyfriend John is super cute I think He is so great My friend just came in and we are going to study the bible with me and Anna  she is a sophomore her name Elanin I have a carpet it is great and she is asking what this nonsense is all about sna I am telling he r that it is for psycology she has a friend that went to Mexico and lived there I was born ',\n",
       "  'vy She my sister has been overseas twice yes to Iraq as wellshe said that where she was was not that bad at all She has now been restationed to florida This makes me really nervous sometimes because we are predicted to have a pretty bad hurricane season this year and many of them hit florida She picked a bad state to go to Anyway I told my sister that she could go wherever she wanted as long as I could come and visit at least once ha Like I could really decide where she would go I just thought Id try it She would have flown me to Iraq to go see the sites and to go see the pyramids in Egypt but my mom would not let me go She did not want the both of us over there However I plan to go visit in Florida either during Christmas break or Spring Break My thoughts have now shifted to the Wells family This is a family or couple that I meet on a plane my eighth grade summer I had gone to National Baptist Conference in Florida and was flying back to houston by myself They told my godmother who was with me before the flight that they would keep an eye on me When we got to houston we exchanged info and said we w',\n",
       "  'I sign up for that class I am doing a pretty good job typing I hope they did not want this in a certain form AOL is such a waste of money I wonder if Jared got me into the draw I hope so I cannot wait till the game on Saturday It will be so much fun I hope his roomate likes me I felt weird today just sitting there I need to make some friends My head hurts so badly I need to go to bed early tonight I do not have to get up half as early tomorrow as I did today I think we are going to have to find a better way of comunicating I was almost in tears when I could not find him I wonder if he was mad at me I hope not I wish he would show more interest in me Hopefully it is just because school started today I guess they call it class and not school I really did not feel overwhelmed today I am so tired Only ten more minutes I feel like I am writing a letter I wonder how Casey and Anh are doing I have not heard from Lauren in about four months She can be ridiculous when she wants to be I wonder if she thinks I moved I hope she does not come here for college i know I would have to be her room mate and everything I did would',\n",
       "  'i will not be pulling my hair out because my employes are idiots plus i will not be working at WalMart the consumer of business Speaking of stuff i love to do I love music I really had a hard decision deciding on rather majoring in music or architecture   but since I have been wanting to do architecture since I was like FOUR i choose it  it all started with legos    On a diffrent note this page is buggy because when you press enter then backspace it returns to the previous page so when you press forward it returns to this page except the COUNTER RESET NOOOOO there goes five minutes of typing luckily i am a very patient person  ltnot very formal hope this is not read yeah right out of a class of 500 please i wa sbearly noticed in my high school class of 680 and i knew people there speaking of people from HIgh Scool one of them is in this class Say hello to Jeff Prudon everyone this is ment to be read in front of class but since it probably will not i might as well TRY to call him out you get your own personl shout out Jeff from the Proffesor too or whoever is reading Jeff is a business major and tries to brag that he will make more money than me   but all i say is I LOVE WHAT I DO and you will not be saying that when you are ready to build your dream house  AND again I will not be having a heartattack because my employes',\n",
       "  'thing I did not want to do Not once We dated for a year and the most he got out of it to brag to his football friends was that we held hands and we kissed each other on each others foreheads and cheeks This was it And God did he do stuff for me I hated saying I needed something or felt a certain way in front of him because he had always go out of his way to helpALWAYS For Valentines Day he designed a shirt that said Monica Will You Be My Valentine I remember that morning walking into the school and knowing that he was going to do something knowing he wanted to celebrate such a special holiday with me I hugged him nodded and smiled That was all he needed I do not know what I would not do to have those feelings all over againwith him I dream about laying beside him while he has his arms wrapped around my shoulders and just watching him breathe watching his chest rise and lower as he breathes WOW Such an amazing sight Now all I have are our memories most of them of me being a bitch to him treating him so unfairly Maybe he does deserve better than me maybe this is all part of Gods plan But you see  I know God knows me and he knows I have a big heart but am just afraid to show it He knows this He knows how so'],\n",
       " 'input_ids': tensor([[    0,    78,     9,  ...,     1,     1,     1],\n",
       "         [    0, 41776,   524,  ...,     1,     1,     1],\n",
       "         [    0,   139,  7476,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   100,  1203,  ...,     1,     1,     1],\n",
       "         [    0,   118,    40,  ...,     1,     1,     1],\n",
       "         [    0, 13247,    38,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([[0., 1., 0., 0., 0.],\n",
       "         [1., 1., 0., 1., 0.],\n",
       "         [1., 0., 0., 1., 0.],\n",
       "         [0., 1., 1., 1., 0.],\n",
       "         [1., 0., 0., 1., 0.],\n",
       "         [1., 1., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [1., 0., 0., 0., 1.],\n",
       "         [0., 0., 1., 1., 0.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 0.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [1., 1., 1., 0., 1.]]),\n",
       " 'tf_idf_features': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0527, 0.0000],\n",
       "         [0.1101, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1184, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c53aac3ae0b448d8bc2a6510d3e1e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "num_tfidf_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "class BERTClass(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = transformers.AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear1 = nn.Linear(768, 256)  # Linear layer for BERT embeddings\n",
    "        self.linear2 = nn.Linear(256 + num_tfidf_features, num_classes)  # Concatenated linear layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids, tfidf_features):\n",
    "        # BERT embeddings\n",
    "        output = self.bert_model(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "        bert_embeddings = output.last_hidden_state.mean(dim=1)  # Example: Mean pooling of token embeddings\n",
    "        bert_embeddings = self.relu(self.linear1(bert_embeddings))  # Apply linear layer and ReLU activation\n",
    "        \n",
    "        # Concatenate with TF-IDF features\n",
    "        concatenated_features = torch.cat((bert_embeddings, tfidf_features), dim=1)\n",
    "        \n",
    "        # Final linear layer for classification\n",
    "        output = self.linear2(self.dropout(concatenated_features))\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "model = BERTClass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=1256, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss combines a Sigmoid layer and the BCELoss in one single class. \n",
    "# This version is more numerically stable than using a plain Sigmoid followed \n",
    "# by a BCELoss as, by combining the operations into one layer, \n",
    "# we take advantage of the log-sum-exp trick for numerical stability.\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model for one epoch\n",
    "def train_model(training_loader, model, optimizer):\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    for batch_ix, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)\n",
    "        tfidf_features = data['tf_idf_features'].to(device, dtype=torch.float)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids,tfidf_features) # output shape: (batch_size, num_classes)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs==targets)\n",
    "        num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Update progress bar\n",
    "        #loop.set_description(f\"\")\n",
    "        #loop.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(validation_loader, model):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for btch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            tfidf_features = data['tf_idf_features'].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids,tfidf_features)\n",
    "\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff9106173fc40b7b5f7000408da23fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6930, val_loss=0.6928 train_acc=0.5136, val_acc=0.5202\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abb6ed071504e57aa8c021cfb234a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6910, val_loss=0.6872 train_acc=0.5364, val_acc=0.5526\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46417f99df446c1a58ae6e221dbbd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6820, val_loss=0.6807 train_acc=0.5609, val_acc=0.5671\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fda06481c10470ca8995d3c940ee65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6691, val_loss=0.6786 train_acc=0.5935, val_acc=0.5666\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab49d15f69044429bb735a0ed823f920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6472, val_loss=0.6833 train_acc=0.6323, val_acc=0.5623\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414606ce968940198f2473e9279a61d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.6231, val_loss=0.6967 train_acc=0.6610, val_acc=0.5596\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de0d214f7604c9b8f3593c25e058eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5975, val_loss=0.6939 train_acc=0.6899, val_acc=0.5687\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8421e6be08864a3ea6b3001fd9fc3e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5690, val_loss=0.7141 train_acc=0.7237, val_acc=0.5601\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7699bd2add214d6cb1eeab50d05801f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5361, val_loss=0.7394 train_acc=0.7474, val_acc=0.5509\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de4866adc69478ebdd93352fd7f2f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.5102, val_loss=0.7446 train_acc=0.7714, val_acc=0.5531\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ff55544bdf4148823d3a93d3d78ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4767, val_loss=0.7777 train_acc=0.8000, val_acc=0.5429\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0ea633a7d74b5780f7d80862bf4406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4502, val_loss=0.7694 train_acc=0.8146, val_acc=0.5520\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b931df9c24c42a795780893efd3671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.4226, val_loss=0.8043 train_acc=0.8367, val_acc=0.5531\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4bb3c1b4644f509948706ca47d9cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.3973, val_loss=0.8117 train_acc=0.8507, val_acc=0.5504\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c40ac04e3943f89e052ac1e116cfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.3669, val_loss=0.8319 train_acc=0.8717, val_acc=0.5612\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc37c2dd5f0f4025a0c24dc078db58c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.3436, val_loss=0.8498 train_acc=0.8812, val_acc=0.5623\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afc9c78c8834266a985d97e897c58b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.3195, val_loss=0.8844 train_acc=0.8943, val_acc=0.5542\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d523504c4e4418aa0d3513fc36fda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.2962, val_loss=0.9233 train_acc=0.9020, val_acc=0.5536\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a0dc298e014406b43a368b54cc8f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.2759, val_loss=0.9494 train_acc=0.9118, val_acc=0.5526\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e573cec83394de4b383de92b12cd9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss=0.2559, val_loss=0.9658 train_acc=0.9250, val_acc=0.5488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    # save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(),\"ckpts/MLTC_roberta_tfidf_model_state.bin\")\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BERTClass:\n\tsize mismatch for linear2.weight: copying a param with shape torch.Size([5, 32260]) from checkpoint, the shape in current model is torch.Size([5, 1256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the best model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BERTClass()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mckpts/MLTC_roberta_model_state.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BERTClass:\n\tsize mismatch for linear2.weight: copying a param with shape torch.Size([5, 32260]) from checkpoint, the shape in current model is torch.Size([5, 1256])."
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model = BERTClass()\n",
    "model.load_state_dict(torch.load(\"ckpts/MLTC_roberta_tfidf_model_state.bin\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5637837837837838, 0.7017953445514044)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_acc, test_loss = eval_model(test_data_loader, model)\n",
    "test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model,data_loader):\n",
    "    model.eval()\n",
    "    text=[]\n",
    "    predictions = []\n",
    "    predictions_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            text.extend(data['text'])\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            tfidf_features = data['tf_idf_features'].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids,tfidf_features)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            predictions.extend(outputs.cpu().detach().round())\n",
    "            real_values.extend(targets.cpu().detach())\n",
    "            predictions_probs.extend(outputs.cpu().detach())      \n",
    "    predictions = torch.stack(predictions)\n",
    "    predictions_probs = torch.stack(predictions_probs)\n",
    "    real_values = torch.stack(real_values)\n",
    "    return text,predictions,predictions_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test data\n",
    "text, predictions, prediction_probs, real_values = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: torch.Size([370, 5]), real values shape: torch.Size([370, 5]), text length: 370, prediction_probs shape: torch.Size([370, 5])\n"
     ]
    }
   ],
   "source": [
    "# print size and shapes\n",
    "print(f\"predictions shape: {predictions.shape}, real values shape: {real_values.shape}, text length: {len(text)}, prediction_probs shape: {prediction_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cEXT       0.52      0.70      0.60       178\n",
      "        cNEU       0.56      0.56      0.56       190\n",
      "        cAGR       0.55      0.82      0.66       195\n",
      "        cCON       0.56      0.85      0.68       186\n",
      "        cOPN       0.63      0.46      0.53       190\n",
      "\n",
      "   micro avg       0.56      0.68      0.61       939\n",
      "   macro avg       0.56      0.68      0.60       939\n",
      "weighted avg       0.56      0.68      0.60       939\n",
      " samples avg       0.56      0.65      0.57       939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(real_values, predictions, target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cEXT': 0.5459459459459459,\n",
       " 'cNEU': 0.5486486486486486,\n",
       " 'cAGR': 0.5486486486486486,\n",
       " 'cCON': 0.5918918918918918,\n",
       " 'cOPN': 0.5837837837837838}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  calulate accuracy using sklearn for each trait\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_scores = {}\n",
    "for i in range(len(target_list)):\n",
    "    accuracy_scores[target_list[i]] = accuracy_score(real_values[:,i], predictions[:,i])\n",
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
